{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/pfrancois/dannyhuang/gfn_test/discrete-gflownet\n",
      "Time taken to run coord_reward_func: 0.000029811 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add the project root directory to sys.path (go up 1 level from notebooks/)\n",
    "# Use os.getcwd() and navigate up since __file__ is not available in notebooks\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    # Navigate up to discrete-gflownet project root (just 1 level up)\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "else:\n",
    "    # Already in project root or somewhere else\n",
    "    project_root = current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root) \n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import numpy as np\n",
    "import time\n",
    "from reward_func.evo_devo import somitogenesis_reward_func, somitogenesis_sol_func, weights_to_matrix, somitogenesis_sparsity_reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 165  175  200 -105  -10   55    5]\n",
      " [ -75 -120  120  -55  105    5   -5]\n",
      " [-185 -165  155  160  100   10   10]\n",
      " [  20  -15  200 -110  155    0   50]\n",
      " [ -15  160   55 -150    5   50    0]\n",
      " [-155   55   -5 -100   10 -150    5]\n",
      " [  50   -5   50   50    0  -50  -50]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b6c8bfc3347b5b66407a600d620a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Cell Position', max=99), FloatSlider(value=165.0, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(cell_pos, **kwargs)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state=[0, 0, 0, 0, 100, 0] # g(i,t)\n",
    "# test_state= [0, 0, -100, 0, 0, 0, -100, -80, 0, 100, 100, 100] # 3n Repressilator \n",
    "# test_state=[70, 50, 10, -30,30, 20] # 2n somite-half\n",
    "# test_state= [85, 50, 10, -80, 40, 20] # 2n somite-s\n",
    "test_state= [100, 0, 40, -100, 30, 20] # 2n somite-s\n",
    "# test_state= [0, 90, 0, 50, 30, 20]  # 2n somite-1\n",
    "# test_state=[100, 100, -60, 50, -75, 30] # 2n chaos\n",
    "\n",
    "# test_state=[37, -89, 88, 89, 76, 51, -56, 43, -57, 35, 1, -16, 36, 7, -53, 6, 0, 0, 31, 0, 32, 0, -51, 0, 31, 6, 56, 1, -50, -2, 1, -32, 1, -30, -5, 0, 80, -100, 58, 75, -50, -50, 0, -30, -75, 76, 100, -26, -5, -39, -18, -36, -25, 51, -61, 1]\n",
    "# test_state=[37, -89, 88, 89, 76, 51, -56, 43, -57, 35, 1, -16, 36, 7, -53, 6, -36, 26, 31, 56, 32, -55, -51, 10, 31, 6, 56, 1, -50, -2, 1, -32, 1, -30, -5, 5, 80, -100, 58, 75, -50, -50, 0, -30, -75, 76, 100, -26, -5, -39, -18, -36, -25, 51, -61, 1]\n",
    "\n",
    "test_state=[160, -50, -110, 105, 5, 0, 50, 50, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -105, 150, 0, 0, 0, 0, 0]\n",
    "test_state=[165, -120, -75, 175, 155, -185, 200, -165, 120, -110, 20, -105, -15, -55, 200, 160, 5, -15, -10, 160, 105, 55, 100, -150, 155, -150, -155, 55, 55, 5, -5, 10, -100, 0, 10, 50, -50, 50, 5, -5, -5, 50, 10, 50, 50, 0, 0, -50, 5, -200, 175, 125, -130, -50, 50, -5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate number of nodes from the length of state vector\n",
    "n_nodes = int((-1 + (1 + 4*len(test_state))**0.5) / 2)  # solve quadratic: n^2 + n - len(state) = 0\n",
    "n_weights = n_nodes * n_nodes\n",
    "\n",
    "W = weights_to_matrix(test_state[:n_weights])\n",
    "print(W)\n",
    "\n",
    "def update_plot(cell_pos, **kwargs):\n",
    "    params = list(kwargs.values())\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 18)) \n",
    "    \n",
    "    # Plot somite pattern and get reward\n",
    "    start_time = time.perf_counter_ns()\n",
    "    # reward = somitogenesis_reward_func(params, plot=True, ax=ax1, SPARSITY_WEIGHT=0.0)\n",
    "    reward = somitogenesis_sparsity_reward_func(params, plot=True, ax=ax1)\n",
    "    end_time = time.perf_counter_ns()\n",
    "    print(f\"Reward for somitogenesis: {reward}\")\n",
    "    print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "    \n",
    "    # Plot oscillation diagram for selected cell_pos\n",
    "    t_sim, cell_trajectory, _ = somitogenesis_sol_func(params, cell_position=cell_pos)\n",
    "    for i in range(n_nodes):\n",
    "        ax2.plot(t_sim, cell_trajectory[:, i], label=f'Gene {i+1}', linewidth=2)\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Gene Concentration')\n",
    "    ax2.set_title(f'Gene Expression Dynamics - Cell {cell_pos}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Draw network motif in last subplot\n",
    "    draw_network_motif(params, ax=ax3)\n",
    "    ax3.set_title(f\"{n_nodes}-Node Network Motif\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create sliders for all parameters\n",
    "sliders = {\n",
    "    'cell_pos': IntSlider(min=0, max=99, step=1, value=1, description='Cell Position')\n",
    "}\n",
    "# Weight sliders\n",
    "for i in range(n_weights):\n",
    "    default_value = test_state[i] if i < len(test_state) else 0\n",
    "    sliders[f'w{i+1}'] = FloatSlider(min=-10000, max=10000, step=1, value=default_value)\n",
    "# D value sliders    \n",
    "for i in range(n_nodes):\n",
    "    default_value = test_state[n_weights+i] if n_weights+i < len(test_state) else 0\n",
    "    sliders[f'd{i+1}'] = FloatSlider(min=-10000, max=10000, step=1, value=default_value)\n",
    "\n",
    "interact(update_plot, **sliders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot network motifs and their corresponding somite patterns\"\"\"\n",
    "# from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# # Define test weights to visualize\n",
    "# test_weights_list = [\n",
    "#     # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "#     [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ]\n",
    "\n",
    "# # Plot network motifs and their corresponding somite patterns\n",
    "# save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "# print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot kymograph of g(i,t) function from somitogenesis reward function\"\"\"\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Parameters from somitogenesis_reward_func\n",
    "# N_CELLS = 100\n",
    "# N_SIMTIME = 90 \n",
    "# N_TIMEPOINTS = 200\n",
    "# A, B = 0.1/5, 0.2/5\n",
    "\n",
    "# # Create position and time arrays\n",
    "# positions = np.arange(N_CELLS).reshape(-1, 1)\n",
    "# t = np.linspace(0, N_SIMTIME, N_TIMEPOINTS)\n",
    "\n",
    "# # Calculate g(i,t) for all positions and times\n",
    "# g_values = np.zeros((N_TIMEPOINTS, N_CELLS))\n",
    "# for i, time in enumerate(t):\n",
    "#     g = np.minimum(np.exp(A * positions - B * time), 1)\n",
    "#     g_values[i] = g.flatten()\n",
    "\n",
    "# # Plot kymograph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(g_values.T, aspect='auto', cmap='Blues', \n",
    "#           extent=[0, N_SIMTIME, N_CELLS, 0])\n",
    "# plt.colorbar(label='g(i,t)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Position')\n",
    "# plt.title('Kymograph of g(i,t) = min(exp(A*i - B*t), 1)')\n",
    "# plt.show()\n",
    "\n",
    "# # Print parameter values used\n",
    "# print(f\"Parameters used:\")\n",
    "# print(f\"A = {A:.6f}\")\n",
    "# print(f\"B = {B:.6f}\")\n",
    "# print(f\"N_CELLS = {N_CELLS}\")\n",
    "# print(f\"N_SIMTIME = {N_SIMTIME}\")\n",
    "# print(f\"N_TIMEPOINTS = {N_TIMEPOINTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test cases for Masking in GridEnv\"\"\"\n",
    "# import sys\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "# # Add the project root directory to sys.path\n",
    "# current_dir = os.getcwd()\n",
    "# if 'notebooks' in current_dir:\n",
    "#     project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "# else:\n",
    "#     project_root = current_dir\n",
    "\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)\n",
    "\n",
    "# from disc_gflownet.envs.grid_env import GridEnv\n",
    "\n",
    "# # Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "# args = SimpleNamespace(\n",
    "#     n_workers=1,\n",
    "#     cache_max_size=1000,\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "#     n_steps=20,\n",
    "#     n_dims=4**2+4,\n",
    "#     max_nodes=4,\n",
    "#     max_edges=2,\n",
    "#     actions_per_dim={'weight': [5, 25, -5, -25], 'diagonal': [5, 25, -5, -25]},\n",
    "#     grid_bound={'weight': {'min': -100, 'max': 100}, 'diagonal': {'min': -100, 'max': 100}},\n",
    "#     enable_time=False,\n",
    "#     consistent_signs=True\n",
    "# )\n",
    "\n",
    "# env = GridEnv(args)\n",
    "\n",
    "# # Print all actions first\n",
    "# env.print_actions()\n",
    "\n",
    "\n",
    "# print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "# print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "# print(\"\\nTesting Masking in GridEnv\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# # s0\n",
    "# env.reset()\n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # First action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s1\n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Second action\n",
    "# action = allowed_indices[3]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s2\n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test cases for Progressive Masking in GridEnv2\"\"\"\n",
    "# import sys\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "# # Add the project root directory to sys.path\n",
    "# current_dir = os.getcwd()\n",
    "# if 'notebooks' in current_dir:\n",
    "#     project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "# else:\n",
    "#     project_root = current_dir\n",
    "\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)\n",
    "\n",
    "# from disc_gflownet.envs.grid_env2 import GridEnv2\n",
    "\n",
    "# # Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "# args = SimpleNamespace(\n",
    "#     n_workers=1,\n",
    "#     cache_max_size=1000,\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "#     actions_per_dim={'weight': [1, 5, 25, -1, -5, -25], 'diagonal': [1, 5, -1, -5]},\n",
    "#     grid_bound={'weight': {'min': -200, 'max': 200}, 'diagonal': {'min': -20, 'max': 20}},\n",
    "#     enable_time=False,\n",
    "#     consistent_signs=True,\n",
    "#     n_dims=3**2+3,  # 9 weights + 3 diagonals\n",
    "#     n_steps=2+6+10,  # Total steps for all network sizes\n",
    "#     steps_per_network={1:2, 2:6, 3:10}  # Steps per network size\n",
    "# )\n",
    "\n",
    "# env = GridEnv2(args)\n",
    "\n",
    "# # Print all actions first\n",
    "# env.print_actions()\n",
    "\n",
    "# print(\"Testing Progressive Masking in GridEnv2\")\n",
    "# print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "# print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "\n",
    "# # s0\n",
    "# env.reset()\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # First action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s1\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Second action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s2\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Third action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s3\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Fourth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s4\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Fifth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s5\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Sixth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s6\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Seventh action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s7\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Eighth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s8\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Ninth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s9\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n",
    "\n",
    "# # Tenth action\n",
    "# action = allowed_indices[0]\n",
    "# next_state, reward, done = env.step(action)\n",
    "# print(\"\\nTaking action:\", action)\n",
    "# print(f\"New state: {env._state}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # s10\n",
    "# print(f\"Current network size: {env.current_network_size}\") \n",
    "# print(f\"Steps Total: {env._step}\")\n",
    "# print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "# mask = env.get_forward_mask(env._state)\n",
    "# print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "# allowed_indices = np.where(mask)[0]\n",
    "# print(f\"Allowed action indices: {allowed_indices}\")\n",
    "# print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test combined sparsity reward\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "def sparsity_reward_combined(state, w1=0.0, w2=1.0):\n",
    "    # Entropy-based component\n",
    "    # Normalize values to probabilities\n",
    "    abs_values = np.abs(state)\n",
    "    if sum(abs_values) == 0:\n",
    "        entropy_reward = 1.0  # maximum sparsity\n",
    "    else:\n",
    "        probs = abs_values / sum(abs_values)\n",
    "        # Calculate entropy (lower entropy = more sparse)\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "        entropy_reward = 1 / (1 + entropy)  # transform to reward\n",
    "    \n",
    "    # L0 component (explicitly rewards zeros)\n",
    "    n_zeros = sum(1 for x in state if x == 0)\n",
    "    l0_reward = n_zeros / len(state)\n",
    "    \n",
    "    return w1 * entropy_reward + w2 * l0_reward\n",
    "\n",
    "# Example states to test\n",
    "sparse_state1 = (10, 1, 1, 1, 10, 1, 1, 1, 10)\n",
    "# sparse_state1 = (10, 0, 0, 0, 10, 0, 0, 0, 10)\n",
    "sparse_state2 = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "\n",
    "SPARSITY_WEIGHT = 5.0\n",
    "sparsity_factor1 = round(1.0 + (SPARSITY_WEIGHT * sparsity_reward_combined(sparse_state1)), 3)\n",
    "sparsity_factor2 = round(1.0 + (SPARSITY_WEIGHT * sparsity_reward_combined(sparse_state2)), 3)\n",
    "\n",
    "\n",
    "# Test combined reward function\n",
    "print(f\"Combined sparsity reward for sparse_state1: { sparsity_factor1 }\")\n",
    "print(f\"Combined sparsity reward for sparse_state2: { sparsity_factor2 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function with overflow protection\"\"\"\n",
    "    return 1 / (1 + np.exp( - np.clip(z, -500, 500)))\n",
    "\n",
    "# Test the sigmoid function with a simple differential equation\n",
    "# dx₁/dt = σ(d₁g(t) + w₁₁x₁ + w₁₂x₂ + w₁₃x₃) - s₁x₁\n",
    "\n",
    "def test_equation(x1, x2, x3, d1, w11, w12, w13, s1, g_value=1.0):\n",
    "    \"\"\"Calculate the right side of the differential equation for x1\"\"\"\n",
    "    z = d1 * g_value + w11 * x1 + w12 * x2 + w13 * x3\n",
    "    return sigmoid(z) - s1 * x1\n",
    "\n",
    "# Create interactive sliders to test the equation\n",
    "def update_equation(**kwargs):\n",
    "    x1 = kwargs['x1']\n",
    "    x2 = kwargs['x2']\n",
    "    x3 = kwargs['x3']\n",
    "    d1 = kwargs['d1']\n",
    "    w11 = kwargs['w11']\n",
    "    w12 = kwargs['w12']\n",
    "    w13 = kwargs['w13']\n",
    "    s1 = kwargs['s1']\n",
    "    g_value = kwargs['g']\n",
    "    \n",
    "    z = d1 * g_value + w11 * x1 + w12 * x2 + w13 * x3\n",
    "    # z= 0\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    dx1dt = sigmoid_z - s1 * x1\n",
    "    \n",
    "    print(f\"z = {z:.4f}\")\n",
    "    print(f\"σ(z) = {sigmoid_z:.4f}\")\n",
    "    # print(f\"dx₁/dt = {dx1dt:.4f}\")\n",
    "    \n",
    "    return sigmoid_z\n",
    "\n",
    "# Create sliders for all parameters\n",
    "equation_sliders = {\n",
    "    'x1': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₁'),\n",
    "    'x2': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₂'),\n",
    "    'x3': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₃'),\n",
    "    'd1': FloatSlider(min=-10, max=10, step=0.5, value=1.0, description='d₁'),\n",
    "    'w11': FloatSlider(min=-10, max=10, step=0.5, value=0.0, description='w₁₁'),\n",
    "    'w12': FloatSlider(min=-10, max=10, step=0.5, value=0.0, description='w₁₂'),\n",
    "    'w13': FloatSlider(min=-10, max=100, step=0.5, value=0.0, description='w₁₃'),\n",
    "    's1': FloatSlider(min=0, max=2, step=0.1, value=1.0, description='s₁'),\n",
    "    'g': FloatSlider(min=0, max=1, step=0.1, value=1.0, description='g(t)')\n",
    "}\n",
    "\n",
    "\n",
    "interact(update_equation, **equation_sliders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test to understand how @ W.T works with a 2-node system\n",
    "n_nodes = 2\n",
    "\n",
    "# Create a simple 2x2 weight matrix\n",
    "W = np.array([\n",
    "    [1, 4],\n",
    "    [3, 2]\n",
    "])\n",
    "print(\"W:\\n\", W)\n",
    "print(\"W.T:\\n\", W.T)\n",
    "\n",
    "# Create a simple x array with 3 cells, 2 nodes each\n",
    "x = np.array([2, 3])  # Flattened array\n",
    "\n",
    "# # Reshape to (3 cells, 2 nodes)\n",
    "x_reshaped = x.reshape(-1, 2)\n",
    "print(\"x_reshaped:\\n\", x_reshaped)\n",
    "\n",
    "# # Calculate x_reshaped @ W.T\n",
    "result = x_reshaped @ W.T\n",
    "print(\"x_reshaped @ W.T:\\n\", result)\n",
    "\n",
    "# Alternative way using W @ x_reshaped.T\n",
    "result_alt = (W @ x_reshaped.T)\n",
    "print(\"\\nAlternative calculation:\")\n",
    "print(\"(W @ x_reshaped.T).T:\\n\", result_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [70, 100, -80, 100, -75, 30]\n",
    "int((-1 + (1 + 4*(6) )**0.5) / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
