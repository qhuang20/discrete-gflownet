{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "import time\n",
    "\n",
    "from disc_gflownet.utils.setting import set_seed, set_device, tf\n",
    "from disc_gflownet.utils.plotting import plot_loss_curve\n",
    "from disc_gflownet.agents.tbflownet_agent import TBFlowNetAgent\n",
    "from disc_gflownet.agents.dbflownet_agent import DBFlowNetAgent\n",
    "from disc_gflownet.envs.grid_env import GridEnv\n",
    "from disc_gflownet.envs.set_env import SetEnv\n",
    "\n",
    "from reward_func.evo_devo import oscillator_reward_func, somitogenesis_reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reward function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom reward function\n",
    "def coord_reward_func(state):\n",
    "    reward1 = sum(1 for coord in state if coord == 3) + 0.001 # args.min_reward\n",
    "    reward2 = sum(1 for coord in state if coord == 5) + 0.002 # args.min_reward\n",
    "    return reward1 + reward2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_weights = [-60, -150, 0, -109, 65, -66, -145, 41, 58] # -41(41), 58\n",
    "reward = somitogenesis_reward_func(test_weights, plot=True)\n",
    "print(f\"Reward for somitogenesis: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "test_weights = [0, -10, 0, 0, 3, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10\n",
    "reward = oscillator_reward_func(test_weights, plot=True)\n",
    "print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid GFN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(args):\n",
    "    global all_losses  \n",
    "    global logZs\n",
    "    global agent\n",
    "\n",
    "    assert args.envsize == args.mbsize\n",
    "    set_seed(args.seed)\n",
    "    set_device(torch.device(args.device))\n",
    "    method_name = args.method\n",
    "    \n",
    "    envs = [GridEnv(args) for _ in range(args.envsize)]\n",
    "    if args.method == 'tb_gfn':\n",
    "        agent = TBFlowNetAgent(args, envs)\n",
    "        opt = torch.optim.Adam([{'params': agent.parameters(), 'lr': args.tb_lr}, {'params':[agent.Z], 'lr': args.tb_z_lr} ])\n",
    "    elif args.method == 'db_gfn' or args.method == 'fldb_gfn':\n",
    "        agent = DBFlowNetAgent(args, envs)\n",
    "        opt = torch.optim.Adam([{'params': agent.parameters(), 'lr': args.tb_lr}])\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"train\"\"\"\n",
    "    \n",
    "    all_losses = [] \n",
    "    logZs = []  # only for tb_gfn\n",
    "    for i in tqdm(range(args.n_train_steps + 1), disable=not args.progress):\n",
    "        experiences = agent.sample_batch_episodes(args.mbsize)\n",
    "        if method_name == 'fldb_gfn':\n",
    "            losses = agent.compute_batch_loss(experiences, use_fldb=True) \n",
    "        else:\n",
    "            losses = agent.compute_batch_loss(experiences) \n",
    "            \n",
    "        all_losses.append(losses[0].item())\n",
    "        logZs.append(losses[1].item()) \n",
    "\n",
    "        losses[0].backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() \n",
    "        \n",
    "        if i % args.log_freq == 0 and args.log_flag:\n",
    "            print(\"\\n\")\n",
    "            print(\"-\"*10)\n",
    "            print(\"Step: \", i)\n",
    "            print(\"Number of unique states found: \", len(agent.ep_last_state_counts))\n",
    "            print(\"Top 12 states by terminal reward:\")\n",
    "            top_states = sorted(agent.ep_last_state_ep_rewards.items(), \n",
    "                              key=lambda x: x[1][-1], # Sort by last reward in trajectory\n",
    "                              reverse=True)[:12]\n",
    "            for state in top_states:\n",
    "                ep_rewards = [f\"{r[0]:.3f}\" for r in agent.ep_last_state_ep_rewards[state[0]]]\n",
    "                count = agent.ep_last_state_counts[state[0]]\n",
    "                print(f\"Full trajectory rewards: {ep_rewards}, Count: {count}, State: {state[0]}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num of threads in use by default: \", torch.get_num_threads())\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "\n",
    "# Set up argparse arguments manually\n",
    "args = argparse.Namespace(\n",
    "    device='cpu',\n",
    "    progress=True,\n",
    "    seed=0,\n",
    "    n_train_steps=1000,  # 2000\n",
    "    log_freq=10,  # 1000\n",
    "    log_flag=True,\n",
    "    mbsize=16,\n",
    "    # Model\n",
    "    method='tb_gfn', \n",
    "    learning_rate=1e-4,\n",
    "    tb_lr=0.001,\n",
    "    tb_z_lr=0.1,\n",
    "    n_hid=256,\n",
    "    n_layers=3,\n",
    "    explore_ratio=0.0,  # 0.0625\n",
    "    temp=1.,\n",
    "    uni_rand_pb=1,\n",
    "    # Env\n",
    "    envsize=16,\n",
    "    min_reward=0.1,\n",
    "    custom_reward_fn=coord_reward_func,\n",
    "    n_steps=50,  \n",
    "    n_dims=2,  \n",
    "    actions_per_dim=[1, 5], # if inhomogenous, need to be a closed symmetrical group.\n",
    "    grid_bound=5\n",
    ")\n",
    "\n",
    "# Call the main function\n",
    "main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Loss and Z for the ({}) Model\".format(args.method)\n",
    "plot_loss_curve(all_losses, logZs=logZs, title=title)\n",
    "print(\"The final Z (partition function) estimate is {:.2f}\".format(np.exp(logZs[-1])))\n",
    "\n",
    "# print(agent.state_counts)\n",
    "# print(agent.state_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"-\"*10) \n",
    "# print(\"Step: \", i)\n",
    "print(\"Number of unique states found: \", len(agent.ep_last_state_counts))\n",
    "print(\"Top 12 states by visit count:\")\n",
    "top_states = sorted(agent.ep_last_state_counts.items(),\n",
    "                    key=lambda x: x[1], # Sort by count\n",
    "                    reverse=True)[:12]\n",
    "for state, count in top_states:\n",
    "    ep_rewards = [f\"{r[0]:.3f}\" for r in agent.ep_last_state_ep_rewards[state]]\n",
    "    print(f\"Count: {count}, State: {state}, Full trajectory rewards: {ep_rewards}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"masking\"\"\"\n",
    "\n",
    "# Test cases for GridEnv with different action types\n",
    "\n",
    "# Test case 1: All positive actions\n",
    "# args_pos = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=custom_reward_func,\n",
    "#     n_steps=7,\n",
    "#     n_dims=2,\n",
    "#     actions_per_dim=[1, 2],  # Only positive actions\n",
    "#     grid_bound=5\n",
    "# )\n",
    "# env_pos = GridEnv(args_pos)\n",
    "# env_pos.print_actions()\n",
    "# env_pos.reset()\n",
    "\n",
    "# for i in range(5):\n",
    "#     obs, reward, done = env_pos.step(0)  # Take step with +1 in dim0\n",
    "#     print(env_pos._state)\n",
    "#     print(env_pos.get_forward_mask(env_pos._state))\n",
    "#     print(\"step: \", env_pos._step)\n",
    "#     print(\"done: \", done)\n",
    "#     print()  # Add blank line between iterations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Test case 2: All negative actions  \n",
    "# args_neg = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=custom_reward_func,\n",
    "#     n_steps=17,\n",
    "#     n_dims=2, \n",
    "#     actions_per_dim=[-1, -2],  # Only negative actions\n",
    "#     grid_bound=5\n",
    "# )\n",
    "# env_neg = GridEnv(args_neg)\n",
    "# env_neg.print_actions()\n",
    "# env_neg.reset()\n",
    "# for i in range(5):\n",
    "#     obs, reward, done = env_neg.step(0)  # Take step with +1 in dim0\n",
    "#     print(env_neg._state)\n",
    "#     print(\"forward: \", env_neg.get_forward_mask(env_neg._state))\n",
    "#     print(\"backward: \", env_neg.get_backward_mask(env_neg._state))\n",
    "#     print(\"step: \", env_neg._step)\n",
    "#     print(\"done: \", done)\n",
    "#     print()  # Add blank line between iterations\n",
    "# for i in range(5):\n",
    "#     obs, reward, done = env_neg.step(2)  # Take step with +1 in dim0\n",
    "#     print(env_neg._state)\n",
    "#     print(\"forward: \", env_neg.get_forward_mask(env_neg._state))\n",
    "#     print(\"backward: \", env_neg.get_backward_mask(env_neg._state))\n",
    "#     print(\"step: \", env_neg._step)\n",
    "#     print(\"done: \", done)\n",
    "#     print()  # Add blank line between iterations\n",
    "\n",
    "\n",
    "\n",
    "# # Test case 3: Mixed positive/negative actions\n",
    "# args_mixed = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=custom_reward_func,\n",
    "#     n_steps=17,\n",
    "#     n_dims=2,\n",
    "#     actions_per_dim=[1, -1, 2, -2],  # Mixed positive and negative actions\n",
    "#     grid_bound=5\n",
    "# )\n",
    "# env_mixed = GridEnv(args_mixed)\n",
    "# env_mixed.print_actions()\n",
    "# env_mixed.reset()\n",
    "\n",
    "# mixed_test_state = [-5, -5]\n",
    "# print(\"forward: \", env_mixed.get_forward_mask(mixed_test_state))\n",
    "# print(\"backward: \", env_mixed.get_backward_mask(mixed_test_state))\n",
    "# print(\"step: \", env_mixed._step)\n",
    "# print(\"done: \", done)\n",
    "# print()  # Add blank line between iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_state = [-3, -1]\n",
    "# print(env_neg.get_forward_mask(test_state))\n",
    "# print(env_neg.get_backward_mask(test_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"encoding\"\"\"\n",
    "# print(env_mixed.encoding_dim)\n",
    "# env_mixed.state_to_encoding([0, 0])\n",
    "\n",
    "# \"\"\"Test encoding for negative actions case\"\"\"\n",
    "# args_neg = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=lambda x: 1.0,  # Simple reward function for testing\n",
    "#     n_steps=10,\n",
    "#     n_dims=2,\n",
    "#     actions_per_dim=[-1, -2],  # Only negative actions\n",
    "#     grid_bound=5\n",
    "# )\n",
    "# env_neg = GridEnv(args_neg)\n",
    "\n",
    "# # Test encoding for negative actions\n",
    "# test_states = [\n",
    "#     [0, 0],      # Origin\n",
    "#     [-1, -1],    # Lower bounds\n",
    "#     [-5, -5],    # Intermediate values\n",
    "#     [-1, -4]     # Mixed negative values\n",
    "# ]\n",
    "\n",
    "# print(\"Testing negative-only action encodings:\")\n",
    "# print(\"-\" * 50)\n",
    "# for state in test_states:\n",
    "#     print(f\"\\nOriginal state: {state}\")\n",
    "#     encoding = env_neg.state_to_encoding(state)\n",
    "#     decoded = env_neg.encoding_to_state(encoding)\n",
    "#     print(f\"Encoding: {encoding}\")\n",
    "#     print(f\"Decoded state: {decoded}\")\n",
    "#     print(f\"Encoding matches: {np.array_equal(state, decoded)}\")\n",
    "\n",
    "# # Verify encoding dimension\n",
    "# print(\"\\nEncoding dimension check:\")\n",
    "# print(f\"Expected dimension: {env_neg.encoding_dim}\")\n",
    "# print(f\"Actual encoding length: {len(env_neg.state_to_encoding(test_states[0]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step function \"\"\"\n",
    "\n",
    "# args_mixed = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=lambda x: 1.0,  # Simple reward function for testing\n",
    "#     n_steps=10,\n",
    "#     n_dims=2,\n",
    "#     actions_per_dim=[-2, -1, 1, 2],  # Mixed positive and negative actions\n",
    "#     grid_bound=5\n",
    "# )\n",
    "# env_mixed = GridEnv(args_mixed)\n",
    "# env_mixed.print_actions()\n",
    "\n",
    "# # Test different step scenarios\n",
    "# print(\"Testing step function with mixed actions:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# # Test case 1: Starting from origin\n",
    "# env_mixed.reset()\n",
    "# print(\"\\nTest Case 1: Starting from origin\")\n",
    "# print(f\"Initial state: {env_mixed._state}\")\n",
    "# action = 0  # First action (-2 in first dimension)\n",
    "# obs, reward, done = env_mixed.step(action)\n",
    "# print(f\"After action {action} (-2 in dim 0):\")\n",
    "# print(f\"New state: {env_mixed._state}\")\n",
    "# print(f\"Reward: {reward}\")\n",
    "# print(f\"Done: {done}\")\n",
    "\n",
    "# # Test case 2: Multiple steps with mixed positive/negative actions\n",
    "# env_mixed.reset()\n",
    "# print(\"\\nTest Case 2: Multiple steps\")\n",
    "# print(f\"Initial state: {env_mixed._state}\")\n",
    "# actions = [0, 6, 3]  # Mix of positive and negative actions in different dimensions\n",
    "# for i, action in enumerate(actions):\n",
    "#     obs, reward, done = env_mixed.step(action)\n",
    "#     print(f\"\\nStep {i+1}, Action {action}:\")\n",
    "#     print(f\"State: {env_mixed._state}\")\n",
    "#     print(f\"Reward: {reward}\")\n",
    "#     print(f\"Done: {done}\")\n",
    "\n",
    "# # Test case 3: Step until done\n",
    "# env_mixed.reset()\n",
    "# print(\"\\nTest Case 3: Step until done\")\n",
    "# print(f\"Initial state: {env_mixed._state}\")\n",
    "# step_count = 0\n",
    "# while True:\n",
    "#     # Get valid actions\n",
    "#     forward_mask = env_mixed.get_forward_mask(env_mixed._state)\n",
    "#     if not np.any(forward_mask):\n",
    "#         print(\"\\nNo valid actions left!\")\n",
    "#         break\n",
    "    \n",
    "#     # Take first valid action\n",
    "#     action = np.where(forward_mask)[0][0]\n",
    "#     obs, reward, done = env_mixed.step(action)\n",
    "#     step_count += 1\n",
    "    \n",
    "#     print(f\"\\nStep {step_count}, Action {action}:\")\n",
    "#     print(f\"State: {env_mixed._state}\")\n",
    "#     print(f\"Reward: {reward}\")\n",
    "#     print(f\"Done: {done}\")\n",
    "    \n",
    "#     if done:\n",
    "#         print(\"\\nEpisode finished!\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
