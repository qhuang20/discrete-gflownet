{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dannyhuang/Developer/gflownet2/base\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n",
    "\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func, somitogenesis_reward_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run coord_reward_func: 0.000026167 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "# test_weights = [0, -10, 0, 4, 6, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10 \n",
    "# test_weights = [154, -200, 82, -33, 90, -90, -82, -13, 30] \n",
    "# reward = oscillator_reward_func(test_weights, plot=True)\n",
    "# print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# test_weights = (126, -125, -56, 107, 105, -126, 100, -11, 175, 1, 1, 1, 1, 1, 1, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25) \n",
    "# test_weights = (-6, -25, 0, 5, -25, 25, 0, 0, 200, 25, -200, 0, 0, 0, -1, 0, 0, 0, 0, 0, 15, 0, 5, 0, 0, 25, 0, 5, 0, 0, 0, -25, -25, 1, 0, 0, 1, -25, 0, 10, 10, 0, -5, -25, 0, 5, 1, -1, 0)\n",
    "\n",
    "test_weights = [60, 32, -38, -85, 70, -63, 22, -27, -7]\n",
    "test_weights = [60, 32, -38, -85, 70, -63, 22, -27, -7, -1, -11, 12, 13, 14, 0, 1]\n",
    "\n",
    "test_weights = [1, 2, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 16]\n",
    "# test_weights = [1, 101, -186, -50, 25, -55, -76, -180, -1]\n",
    "# test_weights = [155, 175, 150, -80, 76, 180, -80, -200, -51]\n",
    "\n",
    "# test_weights = [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 200, 0, -200, -175, 0, 0, 0, -25, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 25, 0, 0, 25, 0, 0]\n",
    "\n",
    "# test_weights = [0, 0, 0, 0, -200, 0, 0, 0, 75, 0, 0, 25, 0, 0, 0, 0, 0, -150, -200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "test_weights = [0, 0, 100, 100]\n",
    "test_weights = [0, 0, 100, 100, -100, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 12))\n",
    "\n",
    "# Plot network motif\n",
    "draw_network_motif(test_weights, ax=ax1)\n",
    "ax1.set_title(\"Network Motif\")\n",
    "\n",
    "# Plot somite pattern and get reward\n",
    "start_time = time.perf_counter_ns()\n",
    "reward = somitogenesis_reward_func(test_weights, plot=True, ax=ax2)\n",
    "print(f\"Reward for somitogenesis: {reward}\")\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# Define test weights to visualize\n",
    "test_weights_list = [\n",
    "    # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "    [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "    [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "    [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Plot network motifs and their corresponding somite patterns\n",
    "save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"LRUCache\"\"\"\n",
    "\n",
    "# import time\n",
    "\n",
    "# # Function that simulates an expensive computation\n",
    "# def expensive_computation(x):\n",
    "#     time.sleep(0.1)  # Simulate work\n",
    "#     return x * 2\n",
    "\n",
    "# # Test without cache\n",
    "# print(\"Without cache:\")\n",
    "# start_time = time.time()\n",
    "# for i in range(10):\n",
    "#     # Compute same values multiple times\n",
    "#     result = expensive_computation(i % 3)  # Only compute 0,1,2 repeatedly\n",
    "#     print(f\"Computing {i % 3} -> {result}\")\n",
    "# no_cache_time = time.time() - start_time\n",
    "# print(f\"Time without cache: {no_cache_time:.2f}s\\n\")\n",
    "\n",
    "# # Test with cache\n",
    "# print(\"With cache:\")\n",
    "# cache = LRUCache(max_size=3)\n",
    "# start_time = time.time()\n",
    "# for i in range(10):\n",
    "#     key = i % 3\n",
    "#     if key in cache:\n",
    "#         result = cache[key]\n",
    "#         print(f\"Cache hit for {key} -> {result}\")\n",
    "#     else:\n",
    "#         result = expensive_computation(key)\n",
    "#         cache[key] = result\n",
    "#         print(f\"Cache miss for {key} -> {result}\")\n",
    "# cache_time = time.time() - start_time\n",
    "# print(f\"Time with cache: {cache_time:.2f}s\")\n",
    "\n",
    "# print(f\"\\nCache speedup: {no_cache_time/cache_time:.1f}x faster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Numba \"\"\"\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "# from numba import njit, prange, set_num_threads, threading_layer\n",
    "# import os\n",
    "\n",
    "# import multiprocessing\n",
    "\n",
    "# print(\"CPU cores available:\", multiprocessing.cpu_count())\n",
    "# set_num_threads(4)  # Manually limit to 4 cores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Slow Python loop\n",
    "# def compute_squares_python(arr):\n",
    "#     return [x**2 for x in arr]\n",
    "\n",
    "# # Fast Numba compiled loop (Single-threaded)\n",
    "# @njit  # Just-in-time compilation\n",
    "# def compute_squares_numba(arr):\n",
    "#     result = np.empty_like(arr)\n",
    "#     for i in range(len(arr)):  # Regular loop (single-threaded)\n",
    "#         result[i] = arr[i] ** 2\n",
    "#     return result\n",
    "\n",
    "# # Faster Numba compiled loop (Parallelized)\n",
    "# @njit(parallel=True)\n",
    "# def compute_squares_parallel(arr):\n",
    "#     result = np.empty_like(arr)\n",
    "#     for i in prange(len(arr)):\n",
    "#         result[i] = arr[i] ** 2\n",
    "#     return result\n",
    "\n",
    "# # Test the speed\n",
    "# arr = np.arange(10**9)\n",
    "\n",
    "# # # Python loop\n",
    "# # start = time.time()\n",
    "# # compute_squares_python(arr)  \n",
    "# # print(\"Python loop time:\", time.time() - start)\n",
    "\n",
    "# # Numba single-threaded\n",
    "# start = time.time()\n",
    "# compute_squares_numba(arr)  \n",
    "# print(\"Numba single-threaded time:\", time.time() - start)\n",
    "\n",
    "# # Numba parallel\n",
    "# start = time.time()\n",
    "# compute_squares_parallel(arr)  \n",
    "# print(\"Numba parallel time:\", time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Test cases for Gray code encoding/decoding\"\"\"\n",
    "\n",
    "# import argparse\n",
    "# from disc_gflownet.envs.grid_env import GridEnv\n",
    "# import numpy as np\n",
    "\n",
    "# # Test cases for Gray code encoding/decoding\n",
    "# args = argparse.Namespace(\n",
    "#     min_reward=0.001,\n",
    "#     custom_reward_fn=coord_reward_func,\n",
    "#     n_workers = 1, \n",
    "#     cache_max_size = 10000, \n",
    "#     consistent_signs = True, \n",
    "#     n_steps=5,\n",
    "#     n_dims=2,  # Testing with 1 dimension\n",
    "#     actions_per_dim=[-1, 1, -10, 10],  # Mixed positive and negative actions\n",
    "#     grid_bound=10,\n",
    "#     enable_time=False\n",
    "# )\n",
    "# env = GridEnv(args)\n",
    "\n",
    "# # Test case 1: Basic encoding/decoding without time\n",
    "# test_state = np.array([10, -10])  # Single dimension state\n",
    "# encoded = env.state_to_encoding(test_state)\n",
    "# decoded = env.encoding_to_state(encoded)\n",
    "# print(\"Test 1 - Basic encoding/decoding:\")\n",
    "# print(f\"Original state: {test_state}\")\n",
    "# print(f\"Encoded: {encoded}\")\n",
    "# print(f\"Decoded state: {decoded}\")\n",
    "# print(f\"Match: {np.array_equal(test_state, decoded)}\\n\")\n",
    "\n",
    "# # # Test case 2: Test one step\n",
    "# # env.reset()\n",
    "# # env.print_actions()\n",
    "# # obs, reward, done = env.step(2) \n",
    "# # print(\"Test 2 - One step:\")\n",
    "# # print(f\"Observation after step: {obs}\")\n",
    "# # print(f\"Current state: {env._state}\\n\")\n",
    "# # obs, reward, done = env.step(6) \n",
    "# # print(\"Test 2 - One step:\")\n",
    "# # print(f\"Observation after step: {obs}\")\n",
    "# # print(f\"Current state: {env._state}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of modes found: 2682\n",
      "\n",
      "Top 10 modes:\n",
      "--------------------------------------------------\n",
      "\n",
      "Mode 1:\n",
      "State: [8, 2, 8, 0, 6, 8, 8, 3, 6]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 163\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 1, 0, 0, 0, 0), Reward=0.001\n",
      "Step 2: State=(1, 0, 0, 0, 1, 0, 0, 0, 0), Reward=0.001\n",
      "Step 3: State=(1, 0, 0, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 4: State=(1, 0, 1, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 5: State=(1, 1, 1, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 6: State=(1, 1, 2, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 7: State=(2, 1, 2, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 8: State=(2, 1, 2, 0, 1, 0, 2, 0, 0), Reward=0.001\n",
      "Step 9: State=(2, 1, 2, 0, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 10: State=(2, 1, 2, 0, 1, 0, 2, 0, 2), Reward=0.001\n",
      "Step 11: State=(2, 1, 2, 0, 1, 0, 2, 0, 3), Reward=0.001\n",
      "Step 12: State=(2, 1, 2, 0, 1, 1, 2, 0, 3), Reward=0.001\n",
      "Step 13: State=(2, 1, 2, 0, 1, 1, 3, 0, 3), Reward=0.001\n",
      "Step 14: State=(2, 1, 2, 0, 1, 2, 3, 0, 3), Reward=0.001\n",
      "Step 15: State=(2, 1, 2, 0, 1, 2, 3, 1, 3), Reward=0.001\n",
      "Step 16: State=(2, 1, 2, 0, 1, 2, 4, 1, 3), Reward=0.001\n",
      "Step 17: State=(2, 1, 2, 0, 2, 2, 4, 1, 3), Reward=0.001\n",
      "Step 18: State=(2, 1, 3, 0, 2, 2, 4, 1, 3), Reward=0.001\n",
      "Step 19: State=(2, 2, 3, 0, 2, 2, 4, 1, 3), Reward=0.001\n",
      "Step 20: State=(2, 2, 3, 0, 2, 3, 4, 1, 3), Reward=0.001\n",
      "Step 21: State=(2, 2, 4, 0, 2, 3, 4, 1, 3), Reward=0.001\n",
      "Step 22: State=(3, 2, 4, 0, 2, 3, 4, 1, 3), Reward=0.001\n",
      "Step 23: State=(3, 2, 4, 0, 2, 3, 4, 1, 4), Reward=0.001\n",
      "Step 24: State=(4, 2, 4, 0, 2, 3, 4, 1, 4), Reward=0.001\n",
      "Step 25: State=(4, 2, 4, 0, 3, 3, 4, 1, 4), Reward=0.001\n",
      "Step 26: State=(4, 2, 4, 0, 3, 3, 5, 1, 4), Reward=0.001\n",
      "Step 27: State=(4, 2, 4, 0, 4, 3, 5, 1, 4), Reward=0.001\n",
      "Step 28: State=(5, 2, 4, 0, 4, 3, 5, 1, 4), Reward=0.001\n",
      "Step 29: State=(5, 2, 4, 0, 4, 3, 6, 1, 4), Reward=1.001\n",
      "Step 30: State=(5, 2, 5, 0, 4, 3, 6, 1, 4), Reward=1.001\n",
      "Step 31: State=(5, 2, 5, 0, 4, 3, 6, 2, 4), Reward=1.001\n",
      "Step 32: State=(5, 2, 5, 0, 5, 3, 6, 2, 4), Reward=1.001\n",
      "Step 33: State=(5, 2, 5, 0, 5, 3, 6, 2, 5), Reward=1.001\n",
      "Step 34: State=(6, 2, 5, 0, 5, 3, 6, 2, 5), Reward=2.001\n",
      "Step 35: State=(6, 2, 5, 0, 5, 3, 6, 2, 6), Reward=3.001\n",
      "Step 36: State=(6, 2, 5, 0, 5, 4, 6, 2, 6), Reward=3.001\n",
      "Step 37: State=(6, 2, 6, 0, 5, 4, 6, 2, 6), Reward=4.001\n",
      "Step 38: State=(6, 2, 6, 0, 5, 5, 6, 2, 6), Reward=4.001\n",
      "Step 39: State=(6, 2, 6, 0, 5, 5, 7, 2, 6), Reward=3.001\n",
      "Step 40: State=(6, 2, 6, 0, 5, 6, 7, 2, 6), Reward=4.001\n",
      "Step 41: State=(6, 2, 6, 0, 5, 6, 7, 3, 6), Reward=4.001\n",
      "Step 42: State=(6, 2, 7, 0, 5, 6, 7, 3, 6), Reward=3.001\n",
      "Step 43: State=(6, 2, 7, 0, 6, 6, 7, 3, 6), Reward=4.001\n",
      "Step 44: State=(7, 2, 7, 0, 6, 6, 7, 3, 6), Reward=3.001\n",
      "Step 45: State=(7, 2, 7, 0, 6, 7, 7, 3, 6), Reward=2.001\n",
      "Step 46: State=(7, 2, 7, 0, 6, 8, 7, 3, 6), Reward=4.001\n",
      "Step 47: State=(7, 2, 8, 0, 6, 8, 7, 3, 6), Reward=6.001\n",
      "Step 48: State=(8, 2, 8, 0, 6, 8, 7, 3, 6), Reward=8.001\n",
      "Step 49: State=(8, 2, 8, 0, 6, 8, 8, 3, 6), Reward=10.001\n",
      "Step 50: State=(8, 2, 8, 0, 6, 8, 8, 3, 7), Reward=9.001\n",
      "Step 51: State=(8, 2, 8, 0, 6, 9, 8, 3, 7), Reward=7.001\n",
      "------------------------------\n",
      "\n",
      "Mode 2:\n",
      "State: [1, 2, 8, 8, 8, 3, 8, 8, 5]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 553\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 0, 0, 1, 0), Reward=0.001\n",
      "Step 2: State=(0, 0, 0, 0, 0, 0, 1, 1, 0), Reward=0.001\n",
      "Step 3: State=(0, 0, 1, 0, 0, 0, 1, 1, 0), Reward=0.001\n",
      "Step 4: State=(0, 0, 1, 0, 1, 0, 1, 1, 0), Reward=0.001\n",
      "Step 5: State=(0, 0, 1, 1, 1, 0, 1, 1, 0), Reward=0.001\n",
      "Step 6: State=(0, 0, 1, 1, 2, 0, 1, 1, 0), Reward=0.001\n",
      "Step 7: State=(0, 0, 1, 1, 2, 0, 1, 2, 0), Reward=0.001\n",
      "Step 8: State=(0, 0, 2, 1, 2, 0, 1, 2, 0), Reward=0.001\n",
      "Step 9: State=(0, 0, 2, 1, 2, 0, 1, 3, 0), Reward=0.001\n",
      "Step 10: State=(0, 0, 2, 1, 2, 0, 1, 4, 0), Reward=0.001\n",
      "Step 11: State=(0, 0, 2, 1, 2, 0, 2, 4, 0), Reward=0.001\n",
      "Step 12: State=(0, 0, 2, 1, 2, 0, 3, 4, 0), Reward=0.001\n",
      "Step 13: State=(0, 0, 2, 2, 2, 0, 3, 4, 0), Reward=0.001\n",
      "Step 14: State=(0, 0, 2, 2, 2, 0, 3, 4, 1), Reward=0.001\n",
      "Step 15: State=(0, 0, 2, 2, 2, 0, 4, 4, 1), Reward=0.001\n",
      "Step 16: State=(0, 0, 2, 2, 2, 0, 4, 4, 2), Reward=0.001\n",
      "Step 17: State=(0, 0, 3, 2, 2, 0, 4, 4, 2), Reward=0.001\n",
      "Step 18: State=(0, 0, 4, 2, 2, 0, 4, 4, 2), Reward=0.001\n",
      "Step 19: State=(0, 0, 4, 2, 3, 0, 4, 4, 2), Reward=0.001\n",
      "Step 20: State=(0, 0, 4, 2, 3, 1, 4, 4, 2), Reward=0.001\n",
      "Step 21: State=(0, 0, 4, 2, 3, 2, 4, 4, 2), Reward=0.001\n",
      "Step 22: State=(0, 0, 4, 2, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 23: State=(0, 0, 4, 3, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 24: State=(0, 0, 4, 4, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 25: State=(1, 0, 4, 4, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 26: State=(1, 1, 4, 4, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 27: State=(1, 1, 4, 5, 4, 2, 4, 4, 2), Reward=0.001\n",
      "Step 28: State=(1, 1, 4, 6, 4, 2, 4, 4, 2), Reward=1.001\n",
      "Step 29: State=(1, 1, 5, 6, 4, 2, 4, 4, 2), Reward=1.001\n",
      "Step 30: State=(1, 1, 5, 6, 4, 2, 5, 4, 2), Reward=1.001\n",
      "Step 31: State=(1, 1, 5, 6, 4, 2, 5, 4, 3), Reward=1.001\n",
      "Step 32: State=(1, 1, 5, 6, 5, 2, 5, 4, 3), Reward=1.001\n",
      "Step 33: State=(1, 1, 5, 6, 5, 2, 6, 4, 3), Reward=2.001\n",
      "Step 34: State=(1, 1, 5, 6, 5, 2, 6, 4, 4), Reward=2.001\n",
      "Step 35: State=(1, 1, 5, 6, 6, 2, 6, 4, 4), Reward=3.001\n",
      "Step 36: State=(1, 1, 5, 6, 7, 2, 6, 4, 4), Reward=2.001\n",
      "Step 37: State=(1, 1, 5, 7, 7, 2, 6, 4, 4), Reward=1.001\n",
      "Step 38: State=(1, 1, 6, 7, 7, 2, 6, 4, 4), Reward=2.001\n",
      "Step 39: State=(1, 2, 6, 7, 7, 2, 6, 4, 4), Reward=2.001\n",
      "Step 40: State=(1, 2, 6, 7, 8, 2, 6, 4, 4), Reward=4.001\n",
      "Step 41: State=(1, 2, 6, 7, 8, 3, 6, 4, 4), Reward=4.001\n",
      "Step 42: State=(1, 2, 6, 7, 8, 3, 6, 5, 4), Reward=4.001\n",
      "Step 43: State=(1, 2, 6, 7, 8, 3, 7, 5, 4), Reward=3.001\n",
      "Step 44: State=(1, 2, 6, 7, 8, 3, 8, 5, 4), Reward=5.001\n",
      "Step 45: State=(1, 2, 6, 7, 8, 3, 8, 6, 4), Reward=6.001\n",
      "Step 46: State=(1, 2, 7, 7, 8, 3, 8, 6, 4), Reward=5.001\n",
      "Step 47: State=(1, 2, 7, 7, 8, 3, 8, 6, 5), Reward=5.001\n",
      "Step 48: State=(1, 2, 7, 7, 8, 3, 8, 7, 5), Reward=4.001\n",
      "Step 49: State=(1, 2, 8, 7, 8, 3, 8, 7, 5), Reward=6.001\n",
      "Step 50: State=(1, 2, 8, 7, 8, 3, 8, 8, 5), Reward=8.001\n",
      "Step 51: State=(1, 2, 8, 8, 8, 3, 8, 8, 5), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 3:\n",
      "State: [8, 3, 2, 8, 8, 3, 8, 3, 8]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 690\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 0, 0, 1, 0), Reward=0.001\n",
      "Step 2: State=(1, 0, 0, 0, 0, 0, 0, 1, 0), Reward=0.001\n",
      "Step 3: State=(1, 0, 0, 0, 0, 1, 0, 1, 0), Reward=0.001\n",
      "Step 4: State=(1, 1, 0, 0, 0, 1, 0, 1, 0), Reward=0.001\n",
      "Step 5: State=(1, 1, 0, 0, 1, 1, 0, 1, 0), Reward=0.001\n",
      "Step 6: State=(1, 1, 0, 0, 1, 1, 1, 1, 0), Reward=0.001\n",
      "Step 7: State=(1, 1, 0, 0, 1, 1, 1, 1, 1), Reward=0.001\n",
      "Step 8: State=(1, 1, 0, 0, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 9: State=(2, 1, 0, 0, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 10: State=(2, 1, 1, 0, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 11: State=(2, 1, 1, 0, 2, 1, 1, 1, 2), Reward=0.001\n",
      "Step 12: State=(2, 1, 1, 0, 2, 1, 1, 1, 3), Reward=0.001\n",
      "Step 13: State=(2, 1, 1, 0, 2, 1, 2, 1, 3), Reward=0.001\n",
      "Step 14: State=(2, 1, 1, 0, 2, 1, 2, 2, 3), Reward=0.001\n",
      "Step 15: State=(2, 1, 1, 1, 2, 1, 2, 2, 3), Reward=0.001\n",
      "Step 16: State=(2, 1, 1, 1, 2, 1, 2, 3, 3), Reward=0.001\n",
      "Step 17: State=(2, 1, 1, 1, 2, 1, 3, 3, 3), Reward=0.001\n",
      "Step 18: State=(2, 2, 1, 1, 2, 1, 3, 3, 3), Reward=0.001\n",
      "Step 19: State=(2, 2, 1, 1, 2, 1, 4, 3, 3), Reward=0.001\n",
      "Step 20: State=(2, 2, 1, 1, 2, 1, 4, 3, 4), Reward=0.001\n",
      "Step 21: State=(3, 2, 1, 1, 2, 1, 4, 3, 4), Reward=0.001\n",
      "Step 22: State=(4, 2, 1, 1, 2, 1, 4, 3, 4), Reward=0.001\n",
      "Step 23: State=(5, 2, 1, 1, 2, 1, 4, 3, 4), Reward=0.001\n",
      "Step 24: State=(5, 2, 1, 1, 3, 1, 4, 3, 4), Reward=0.001\n",
      "Step 25: State=(5, 2, 1, 2, 3, 1, 4, 3, 4), Reward=0.001\n",
      "Step 26: State=(5, 2, 1, 2, 4, 1, 4, 3, 4), Reward=0.001\n",
      "Step 27: State=(5, 2, 1, 2, 4, 1, 5, 3, 4), Reward=0.001\n",
      "Step 28: State=(5, 3, 1, 2, 4, 1, 5, 3, 4), Reward=0.001\n",
      "Step 29: State=(5, 3, 1, 2, 5, 1, 5, 3, 4), Reward=0.001\n",
      "Step 30: State=(5, 3, 1, 2, 5, 1, 5, 3, 5), Reward=0.001\n",
      "Step 31: State=(5, 3, 1, 2, 5, 1, 5, 3, 6), Reward=1.001\n",
      "Step 32: State=(5, 3, 1, 2, 5, 1, 6, 3, 6), Reward=2.001\n",
      "Step 33: State=(5, 3, 1, 2, 5, 1, 7, 3, 6), Reward=1.001\n",
      "Step 34: State=(5, 3, 1, 2, 6, 1, 7, 3, 6), Reward=2.001\n",
      "Step 35: State=(5, 3, 2, 2, 6, 1, 7, 3, 6), Reward=2.001\n",
      "Step 36: State=(5, 3, 2, 2, 6, 2, 7, 3, 6), Reward=2.001\n",
      "Step 37: State=(5, 3, 2, 2, 6, 3, 7, 3, 6), Reward=2.001\n",
      "Step 38: State=(5, 3, 2, 3, 6, 3, 7, 3, 6), Reward=2.001\n",
      "Step 39: State=(5, 3, 2, 3, 7, 3, 7, 3, 6), Reward=1.001\n",
      "Step 40: State=(6, 3, 2, 3, 7, 3, 7, 3, 6), Reward=2.001\n",
      "Step 41: State=(6, 3, 2, 4, 7, 3, 7, 3, 6), Reward=2.001\n",
      "Step 42: State=(6, 3, 2, 4, 7, 3, 8, 3, 6), Reward=4.001\n",
      "Step 43: State=(7, 3, 2, 4, 7, 3, 8, 3, 6), Reward=3.001\n",
      "Step 44: State=(8, 3, 2, 4, 7, 3, 8, 3, 6), Reward=5.001\n",
      "Step 45: State=(8, 3, 2, 5, 7, 3, 8, 3, 6), Reward=5.001\n",
      "Step 46: State=(8, 3, 2, 5, 7, 3, 8, 3, 7), Reward=4.001\n",
      "Step 47: State=(8, 3, 2, 6, 7, 3, 8, 3, 7), Reward=5.001\n",
      "Step 48: State=(8, 3, 2, 6, 8, 3, 8, 3, 7), Reward=7.001\n",
      "Step 49: State=(8, 3, 2, 7, 8, 3, 8, 3, 7), Reward=6.001\n",
      "Step 50: State=(8, 3, 2, 8, 8, 3, 8, 3, 7), Reward=8.001\n",
      "Step 51: State=(8, 3, 2, 8, 8, 3, 8, 3, 8), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 4:\n",
      "State: [8, 4, 8, 8, 3, 2, 8, 2, 8]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1043\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(1, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.001\n",
      "Step 2: State=(1, 0, 0, 0, 0, 0, 0, 0, 1), Reward=0.001\n",
      "Step 3: State=(1, 0, 0, 1, 0, 0, 0, 0, 1), Reward=0.001\n",
      "Step 4: State=(1, 0, 1, 1, 0, 0, 0, 0, 1), Reward=0.001\n",
      "Step 5: State=(1, 0, 1, 1, 0, 0, 0, 0, 2), Reward=0.001\n",
      "Step 6: State=(1, 0, 1, 2, 0, 0, 0, 0, 2), Reward=0.001\n",
      "Step 7: State=(1, 0, 1, 2, 0, 0, 1, 0, 2), Reward=0.001\n",
      "Step 8: State=(1, 0, 1, 2, 0, 0, 2, 0, 2), Reward=0.001\n",
      "Step 9: State=(2, 0, 1, 2, 0, 0, 2, 0, 2), Reward=0.001\n",
      "Step 10: State=(3, 0, 1, 2, 0, 0, 2, 0, 2), Reward=0.001\n",
      "Step 11: State=(3, 0, 2, 2, 0, 0, 2, 0, 2), Reward=0.001\n",
      "Step 12: State=(3, 0, 2, 3, 0, 0, 2, 0, 2), Reward=0.001\n",
      "Step 13: State=(3, 0, 2, 3, 1, 0, 2, 0, 2), Reward=0.001\n",
      "Step 14: State=(3, 0, 2, 3, 1, 0, 2, 1, 2), Reward=0.001\n",
      "Step 15: State=(3, 0, 2, 3, 1, 1, 2, 1, 2), Reward=0.001\n",
      "Step 16: State=(3, 0, 3, 3, 1, 1, 2, 1, 2), Reward=0.001\n",
      "Step 17: State=(3, 0, 3, 3, 1, 2, 2, 1, 2), Reward=0.001\n",
      "Step 18: State=(3, 0, 3, 3, 1, 2, 3, 1, 2), Reward=0.001\n",
      "Step 19: State=(3, 0, 3, 3, 1, 2, 3, 1, 3), Reward=0.001\n",
      "Step 20: State=(3, 0, 3, 3, 2, 2, 3, 1, 3), Reward=0.001\n",
      "Step 21: State=(3, 0, 3, 3, 2, 2, 3, 1, 4), Reward=0.001\n",
      "Step 22: State=(3, 0, 4, 3, 2, 2, 3, 1, 4), Reward=0.001\n",
      "Step 23: State=(4, 0, 4, 3, 2, 2, 3, 1, 4), Reward=0.001\n",
      "Step 24: State=(4, 0, 4, 3, 2, 2, 4, 1, 4), Reward=0.001\n",
      "Step 25: State=(5, 0, 4, 3, 2, 2, 4, 1, 4), Reward=0.001\n",
      "Step 26: State=(6, 0, 4, 3, 2, 2, 4, 1, 4), Reward=1.001\n",
      "Step 27: State=(6, 0, 4, 4, 2, 2, 4, 1, 4), Reward=1.001\n",
      "Step 28: State=(6, 0, 4, 5, 2, 2, 4, 1, 4), Reward=1.001\n",
      "Step 29: State=(6, 1, 4, 5, 2, 2, 4, 1, 4), Reward=1.001\n",
      "Step 30: State=(6, 1, 4, 5, 2, 2, 4, 1, 5), Reward=1.001\n",
      "Step 31: State=(6, 2, 4, 5, 2, 2, 4, 1, 5), Reward=1.001\n",
      "Step 32: State=(6, 3, 4, 5, 2, 2, 4, 1, 5), Reward=1.001\n",
      "Step 33: State=(6, 3, 4, 5, 2, 2, 5, 1, 5), Reward=1.001\n",
      "Step 34: State=(6, 3, 5, 5, 2, 2, 5, 1, 5), Reward=1.001\n",
      "Step 35: State=(7, 3, 5, 5, 2, 2, 5, 1, 5), Reward=0.001\n",
      "Step 36: State=(7, 3, 5, 5, 2, 2, 6, 1, 5), Reward=1.001\n",
      "Step 37: State=(7, 3, 5, 5, 2, 2, 7, 1, 5), Reward=0.001\n",
      "Step 38: State=(7, 3, 5, 5, 2, 2, 7, 1, 6), Reward=1.001\n",
      "Step 39: State=(7, 3, 6, 5, 2, 2, 7, 1, 6), Reward=2.001\n",
      "Step 40: State=(7, 4, 6, 5, 2, 2, 7, 1, 6), Reward=2.001\n",
      "Step 41: State=(8, 4, 6, 5, 2, 2, 7, 1, 6), Reward=4.001\n",
      "Step 42: State=(8, 4, 7, 5, 2, 2, 7, 1, 6), Reward=3.001\n",
      "Step 43: State=(8, 4, 7, 5, 2, 2, 7, 1, 7), Reward=2.001\n",
      "Step 44: State=(8, 4, 7, 5, 3, 2, 7, 1, 7), Reward=2.001\n",
      "Step 45: State=(8, 4, 7, 5, 3, 2, 7, 1, 8), Reward=4.001\n",
      "Step 46: State=(8, 4, 7, 5, 3, 2, 7, 2, 8), Reward=4.001\n",
      "Step 47: State=(8, 4, 7, 5, 3, 2, 8, 2, 8), Reward=6.001\n",
      "Step 48: State=(8, 4, 8, 5, 3, 2, 8, 2, 8), Reward=8.001\n",
      "Step 49: State=(8, 4, 8, 6, 3, 2, 8, 2, 8), Reward=9.001\n",
      "Step 50: State=(8, 4, 8, 7, 3, 2, 8, 2, 8), Reward=8.001\n",
      "Step 51: State=(8, 4, 8, 8, 3, 2, 8, 2, 8), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 5:\n",
      "State: [1, 6, 6, 1, 6, 8, 8, 8, 6]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1398\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 1, 0, 0, 0, 0, 0, 0), Reward=0.001\n",
      "Step 2: State=(0, 0, 1, 0, 0, 1, 0, 0, 0), Reward=0.001\n",
      "Step 3: State=(0, 1, 1, 0, 0, 1, 0, 0, 0), Reward=0.001\n",
      "Step 4: State=(0, 1, 1, 0, 0, 1, 1, 0, 0), Reward=0.001\n",
      "Step 5: State=(0, 1, 2, 0, 0, 1, 1, 0, 0), Reward=0.001\n",
      "Step 6: State=(0, 1, 2, 0, 0, 1, 2, 0, 0), Reward=0.001\n",
      "Step 7: State=(0, 1, 2, 0, 1, 1, 2, 0, 0), Reward=0.001\n",
      "Step 8: State=(0, 2, 2, 0, 1, 1, 2, 0, 0), Reward=0.001\n",
      "Step 9: State=(0, 2, 2, 0, 1, 2, 2, 0, 0), Reward=0.001\n",
      "Step 10: State=(0, 2, 2, 0, 1, 2, 2, 1, 0), Reward=0.001\n",
      "Step 11: State=(0, 2, 2, 0, 1, 3, 2, 1, 0), Reward=0.001\n",
      "Step 12: State=(0, 2, 2, 0, 1, 3, 2, 2, 0), Reward=0.001\n",
      "Step 13: State=(0, 2, 2, 0, 1, 3, 2, 3, 0), Reward=0.001\n",
      "Step 14: State=(0, 2, 2, 0, 1, 3, 3, 3, 0), Reward=0.001\n",
      "Step 15: State=(0, 2, 2, 0, 1, 4, 3, 3, 0), Reward=0.001\n",
      "Step 16: State=(0, 2, 2, 0, 1, 4, 3, 4, 0), Reward=0.001\n",
      "Step 17: State=(0, 2, 2, 0, 1, 5, 3, 4, 0), Reward=0.001\n",
      "Step 18: State=(0, 2, 2, 0, 1, 5, 3, 4, 1), Reward=0.001\n",
      "Step 19: State=(0, 2, 2, 0, 1, 5, 3, 5, 1), Reward=0.001\n",
      "Step 20: State=(0, 2, 2, 1, 1, 5, 3, 5, 1), Reward=0.001\n",
      "Step 21: State=(0, 2, 2, 1, 2, 5, 3, 5, 1), Reward=0.001\n",
      "Step 22: State=(0, 2, 2, 1, 2, 6, 3, 5, 1), Reward=1.001\n",
      "Step 23: State=(0, 2, 3, 1, 2, 6, 3, 5, 1), Reward=1.001\n",
      "Step 24: State=(0, 2, 3, 1, 2, 6, 3, 5, 2), Reward=1.001\n",
      "Step 25: State=(0, 2, 3, 1, 2, 7, 3, 5, 2), Reward=0.001\n",
      "Step 26: State=(0, 2, 3, 1, 2, 8, 3, 5, 2), Reward=2.001\n",
      "Step 27: State=(0, 2, 3, 1, 2, 8, 3, 6, 2), Reward=3.001\n",
      "Step 28: State=(0, 2, 3, 1, 2, 8, 4, 6, 2), Reward=3.001\n",
      "Step 29: State=(0, 2, 3, 1, 2, 8, 5, 6, 2), Reward=3.001\n",
      "Step 30: State=(0, 2, 3, 1, 2, 8, 5, 7, 2), Reward=2.001\n",
      "Step 31: State=(0, 2, 3, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 32: State=(0, 2, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 33: State=(0, 3, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 34: State=(1, 3, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 35: State=(1, 3, 4, 1, 2, 8, 6, 7, 3), Reward=3.001\n",
      "Step 36: State=(1, 3, 4, 1, 2, 8, 6, 7, 4), Reward=3.001\n",
      "Step 37: State=(1, 3, 5, 1, 2, 8, 6, 7, 4), Reward=3.001\n",
      "Step 38: State=(1, 3, 5, 1, 3, 8, 6, 7, 4), Reward=3.001\n",
      "Step 39: State=(1, 3, 5, 1, 3, 8, 6, 7, 5), Reward=3.001\n",
      "Step 40: State=(1, 3, 6, 1, 3, 8, 6, 7, 5), Reward=4.001\n",
      "Step 41: State=(1, 3, 6, 1, 3, 8, 6, 8, 5), Reward=6.001\n",
      "Step 42: State=(1, 3, 6, 1, 3, 8, 6, 8, 6), Reward=7.001\n",
      "Step 43: State=(1, 3, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 44: State=(1, 4, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 45: State=(1, 5, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 46: State=(1, 5, 6, 1, 4, 8, 7, 8, 6), Reward=6.001\n",
      "Step 47: State=(1, 5, 6, 1, 4, 8, 8, 8, 6), Reward=8.001\n",
      "Step 48: State=(1, 5, 6, 1, 5, 8, 8, 8, 6), Reward=8.001\n",
      "Step 49: State=(1, 6, 6, 1, 5, 8, 8, 8, 6), Reward=9.001\n",
      "Step 50: State=(1, 6, 6, 1, 6, 8, 8, 8, 6), Reward=10.001\n",
      "Step 51: State=(2, 6, 6, 1, 6, 8, 8, 8, 6), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 6:\n",
      "State: [2, 6, 6, 1, 6, 8, 8, 8, 6]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1398\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 1, 0, 0, 0, 0, 0, 0), Reward=0.001\n",
      "Step 2: State=(0, 0, 1, 0, 0, 1, 0, 0, 0), Reward=0.001\n",
      "Step 3: State=(0, 1, 1, 0, 0, 1, 0, 0, 0), Reward=0.001\n",
      "Step 4: State=(0, 1, 1, 0, 0, 1, 1, 0, 0), Reward=0.001\n",
      "Step 5: State=(0, 1, 2, 0, 0, 1, 1, 0, 0), Reward=0.001\n",
      "Step 6: State=(0, 1, 2, 0, 0, 1, 2, 0, 0), Reward=0.001\n",
      "Step 7: State=(0, 1, 2, 0, 1, 1, 2, 0, 0), Reward=0.001\n",
      "Step 8: State=(0, 2, 2, 0, 1, 1, 2, 0, 0), Reward=0.001\n",
      "Step 9: State=(0, 2, 2, 0, 1, 2, 2, 0, 0), Reward=0.001\n",
      "Step 10: State=(0, 2, 2, 0, 1, 2, 2, 1, 0), Reward=0.001\n",
      "Step 11: State=(0, 2, 2, 0, 1, 3, 2, 1, 0), Reward=0.001\n",
      "Step 12: State=(0, 2, 2, 0, 1, 3, 2, 2, 0), Reward=0.001\n",
      "Step 13: State=(0, 2, 2, 0, 1, 3, 2, 3, 0), Reward=0.001\n",
      "Step 14: State=(0, 2, 2, 0, 1, 3, 3, 3, 0), Reward=0.001\n",
      "Step 15: State=(0, 2, 2, 0, 1, 4, 3, 3, 0), Reward=0.001\n",
      "Step 16: State=(0, 2, 2, 0, 1, 4, 3, 4, 0), Reward=0.001\n",
      "Step 17: State=(0, 2, 2, 0, 1, 5, 3, 4, 0), Reward=0.001\n",
      "Step 18: State=(0, 2, 2, 0, 1, 5, 3, 4, 1), Reward=0.001\n",
      "Step 19: State=(0, 2, 2, 0, 1, 5, 3, 5, 1), Reward=0.001\n",
      "Step 20: State=(0, 2, 2, 1, 1, 5, 3, 5, 1), Reward=0.001\n",
      "Step 21: State=(0, 2, 2, 1, 2, 5, 3, 5, 1), Reward=0.001\n",
      "Step 22: State=(0, 2, 2, 1, 2, 6, 3, 5, 1), Reward=1.001\n",
      "Step 23: State=(0, 2, 3, 1, 2, 6, 3, 5, 1), Reward=1.001\n",
      "Step 24: State=(0, 2, 3, 1, 2, 6, 3, 5, 2), Reward=1.001\n",
      "Step 25: State=(0, 2, 3, 1, 2, 7, 3, 5, 2), Reward=0.001\n",
      "Step 26: State=(0, 2, 3, 1, 2, 8, 3, 5, 2), Reward=2.001\n",
      "Step 27: State=(0, 2, 3, 1, 2, 8, 3, 6, 2), Reward=3.001\n",
      "Step 28: State=(0, 2, 3, 1, 2, 8, 4, 6, 2), Reward=3.001\n",
      "Step 29: State=(0, 2, 3, 1, 2, 8, 5, 6, 2), Reward=3.001\n",
      "Step 30: State=(0, 2, 3, 1, 2, 8, 5, 7, 2), Reward=2.001\n",
      "Step 31: State=(0, 2, 3, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 32: State=(0, 2, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 33: State=(0, 3, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 34: State=(1, 3, 4, 1, 2, 8, 5, 7, 3), Reward=2.001\n",
      "Step 35: State=(1, 3, 4, 1, 2, 8, 6, 7, 3), Reward=3.001\n",
      "Step 36: State=(1, 3, 4, 1, 2, 8, 6, 7, 4), Reward=3.001\n",
      "Step 37: State=(1, 3, 5, 1, 2, 8, 6, 7, 4), Reward=3.001\n",
      "Step 38: State=(1, 3, 5, 1, 3, 8, 6, 7, 4), Reward=3.001\n",
      "Step 39: State=(1, 3, 5, 1, 3, 8, 6, 7, 5), Reward=3.001\n",
      "Step 40: State=(1, 3, 6, 1, 3, 8, 6, 7, 5), Reward=4.001\n",
      "Step 41: State=(1, 3, 6, 1, 3, 8, 6, 8, 5), Reward=6.001\n",
      "Step 42: State=(1, 3, 6, 1, 3, 8, 6, 8, 6), Reward=7.001\n",
      "Step 43: State=(1, 3, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 44: State=(1, 4, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 45: State=(1, 5, 6, 1, 4, 8, 6, 8, 6), Reward=7.001\n",
      "Step 46: State=(1, 5, 6, 1, 4, 8, 7, 8, 6), Reward=6.001\n",
      "Step 47: State=(1, 5, 6, 1, 4, 8, 8, 8, 6), Reward=8.001\n",
      "Step 48: State=(1, 5, 6, 1, 5, 8, 8, 8, 6), Reward=8.001\n",
      "Step 49: State=(1, 6, 6, 1, 5, 8, 8, 8, 6), Reward=9.001\n",
      "Step 50: State=(1, 6, 6, 1, 6, 8, 8, 8, 6), Reward=10.001\n",
      "Step 51: State=(2, 6, 6, 1, 6, 8, 8, 8, 6), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 7:\n",
      "State: [8, 3, 6, 8, 2, 8, 6, 1, 8]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1460\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 2: State=(1, 0, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 3: State=(1, 1, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 4: State=(2, 1, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 5: State=(2, 1, 0, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 6: State=(3, 1, 0, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 7: State=(3, 1, 0, 0, 1, 0, 2, 0, 0), Reward=0.001\n",
      "Step 8: State=(3, 1, 0, 0, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 9: State=(3, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 10: State=(4, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 11: State=(5, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 12: State=(5, 1, 0, 1, 1, 1, 2, 0, 1), Reward=0.001\n",
      "Step 13: State=(5, 1, 0, 1, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 14: State=(5, 1, 0, 2, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 15: State=(5, 1, 0, 3, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 16: State=(5, 1, 1, 3, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 17: State=(5, 1, 1, 3, 1, 2, 2, 1, 1), Reward=0.001\n",
      "Step 18: State=(5, 1, 1, 3, 1, 3, 2, 1, 1), Reward=0.001\n",
      "Step 19: State=(5, 1, 1, 3, 1, 4, 2, 1, 1), Reward=0.001\n",
      "Step 20: State=(5, 1, 1, 3, 1, 4, 2, 1, 2), Reward=0.001\n",
      "Step 21: State=(5, 1, 1, 3, 1, 4, 3, 1, 2), Reward=0.001\n",
      "Step 22: State=(5, 2, 1, 3, 1, 4, 3, 1, 2), Reward=0.001\n",
      "Step 23: State=(5, 2, 1, 3, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 24: State=(5, 2, 2, 3, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 25: State=(5, 2, 2, 4, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 26: State=(5, 2, 2, 4, 1, 6, 3, 1, 2), Reward=1.001\n",
      "Step 27: State=(5, 2, 2, 4, 1, 6, 3, 1, 3), Reward=1.001\n",
      "Step 28: State=(5, 2, 2, 4, 1, 6, 3, 1, 4), Reward=1.001\n",
      "Step 29: State=(5, 2, 2, 5, 1, 6, 3, 1, 4), Reward=1.001\n",
      "Step 30: State=(5, 2, 2, 5, 1, 7, 3, 1, 4), Reward=0.001\n",
      "Step 31: State=(5, 3, 2, 5, 1, 7, 3, 1, 4), Reward=0.001\n",
      "Step 32: State=(5, 3, 2, 5, 2, 7, 3, 1, 4), Reward=0.001\n",
      "Step 33: State=(6, 3, 2, 5, 2, 7, 3, 1, 4), Reward=1.001\n",
      "Step 34: State=(6, 3, 2, 5, 2, 7, 3, 1, 5), Reward=1.001\n",
      "Step 35: State=(7, 3, 2, 5, 2, 7, 3, 1, 5), Reward=0.001\n",
      "Step 36: State=(7, 3, 2, 5, 2, 7, 3, 1, 6), Reward=1.001\n",
      "Step 37: State=(7, 3, 3, 5, 2, 7, 3, 1, 6), Reward=1.001\n",
      "Step 38: State=(7, 3, 3, 5, 2, 7, 3, 1, 7), Reward=0.001\n",
      "Step 39: State=(8, 3, 3, 5, 2, 7, 3, 1, 7), Reward=2.001\n",
      "Step 40: State=(8, 3, 3, 5, 2, 7, 4, 1, 7), Reward=2.001\n",
      "Step 41: State=(8, 3, 4, 5, 2, 7, 4, 1, 7), Reward=2.001\n",
      "Step 42: State=(8, 3, 4, 5, 2, 7, 5, 1, 7), Reward=2.001\n",
      "Step 43: State=(8, 3, 4, 6, 2, 7, 5, 1, 7), Reward=3.001\n",
      "Step 44: State=(8, 3, 4, 6, 2, 7, 5, 1, 8), Reward=5.001\n",
      "Step 45: State=(8, 3, 4, 6, 2, 8, 5, 1, 8), Reward=7.001\n",
      "Step 46: State=(8, 3, 4, 7, 2, 8, 5, 1, 8), Reward=6.001\n",
      "Step 47: State=(8, 3, 5, 7, 2, 8, 5, 1, 8), Reward=6.001\n",
      "Step 48: State=(8, 3, 5, 8, 2, 8, 5, 1, 8), Reward=8.001\n",
      "Step 49: State=(8, 3, 5, 8, 2, 8, 6, 1, 8), Reward=9.001\n",
      "Step 50: State=(8, 3, 6, 8, 2, 8, 6, 1, 8), Reward=10.001\n",
      "Step 51: State=(8, 3, 6, 8, 2, 8, 6, 2, 8), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 8:\n",
      "State: [8, 3, 6, 8, 2, 8, 6, 2, 8]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1460\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 2: State=(1, 0, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 3: State=(1, 1, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 4: State=(2, 1, 0, 0, 0, 0, 1, 0, 0), Reward=0.001\n",
      "Step 5: State=(2, 1, 0, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 6: State=(3, 1, 0, 0, 1, 0, 1, 0, 0), Reward=0.001\n",
      "Step 7: State=(3, 1, 0, 0, 1, 0, 2, 0, 0), Reward=0.001\n",
      "Step 8: State=(3, 1, 0, 0, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 9: State=(3, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 10: State=(4, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 11: State=(5, 1, 0, 1, 1, 0, 2, 0, 1), Reward=0.001\n",
      "Step 12: State=(5, 1, 0, 1, 1, 1, 2, 0, 1), Reward=0.001\n",
      "Step 13: State=(5, 1, 0, 1, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 14: State=(5, 1, 0, 2, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 15: State=(5, 1, 0, 3, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 16: State=(5, 1, 1, 3, 1, 2, 2, 0, 1), Reward=0.001\n",
      "Step 17: State=(5, 1, 1, 3, 1, 2, 2, 1, 1), Reward=0.001\n",
      "Step 18: State=(5, 1, 1, 3, 1, 3, 2, 1, 1), Reward=0.001\n",
      "Step 19: State=(5, 1, 1, 3, 1, 4, 2, 1, 1), Reward=0.001\n",
      "Step 20: State=(5, 1, 1, 3, 1, 4, 2, 1, 2), Reward=0.001\n",
      "Step 21: State=(5, 1, 1, 3, 1, 4, 3, 1, 2), Reward=0.001\n",
      "Step 22: State=(5, 2, 1, 3, 1, 4, 3, 1, 2), Reward=0.001\n",
      "Step 23: State=(5, 2, 1, 3, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 24: State=(5, 2, 2, 3, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 25: State=(5, 2, 2, 4, 1, 5, 3, 1, 2), Reward=0.001\n",
      "Step 26: State=(5, 2, 2, 4, 1, 6, 3, 1, 2), Reward=1.001\n",
      "Step 27: State=(5, 2, 2, 4, 1, 6, 3, 1, 3), Reward=1.001\n",
      "Step 28: State=(5, 2, 2, 4, 1, 6, 3, 1, 4), Reward=1.001\n",
      "Step 29: State=(5, 2, 2, 5, 1, 6, 3, 1, 4), Reward=1.001\n",
      "Step 30: State=(5, 2, 2, 5, 1, 7, 3, 1, 4), Reward=0.001\n",
      "Step 31: State=(5, 3, 2, 5, 1, 7, 3, 1, 4), Reward=0.001\n",
      "Step 32: State=(5, 3, 2, 5, 2, 7, 3, 1, 4), Reward=0.001\n",
      "Step 33: State=(6, 3, 2, 5, 2, 7, 3, 1, 4), Reward=1.001\n",
      "Step 34: State=(6, 3, 2, 5, 2, 7, 3, 1, 5), Reward=1.001\n",
      "Step 35: State=(7, 3, 2, 5, 2, 7, 3, 1, 5), Reward=0.001\n",
      "Step 36: State=(7, 3, 2, 5, 2, 7, 3, 1, 6), Reward=1.001\n",
      "Step 37: State=(7, 3, 3, 5, 2, 7, 3, 1, 6), Reward=1.001\n",
      "Step 38: State=(7, 3, 3, 5, 2, 7, 3, 1, 7), Reward=0.001\n",
      "Step 39: State=(8, 3, 3, 5, 2, 7, 3, 1, 7), Reward=2.001\n",
      "Step 40: State=(8, 3, 3, 5, 2, 7, 4, 1, 7), Reward=2.001\n",
      "Step 41: State=(8, 3, 4, 5, 2, 7, 4, 1, 7), Reward=2.001\n",
      "Step 42: State=(8, 3, 4, 5, 2, 7, 5, 1, 7), Reward=2.001\n",
      "Step 43: State=(8, 3, 4, 6, 2, 7, 5, 1, 7), Reward=3.001\n",
      "Step 44: State=(8, 3, 4, 6, 2, 7, 5, 1, 8), Reward=5.001\n",
      "Step 45: State=(8, 3, 4, 6, 2, 8, 5, 1, 8), Reward=7.001\n",
      "Step 46: State=(8, 3, 4, 7, 2, 8, 5, 1, 8), Reward=6.001\n",
      "Step 47: State=(8, 3, 5, 7, 2, 8, 5, 1, 8), Reward=6.001\n",
      "Step 48: State=(8, 3, 5, 8, 2, 8, 5, 1, 8), Reward=8.001\n",
      "Step 49: State=(8, 3, 5, 8, 2, 8, 6, 1, 8), Reward=9.001\n",
      "Step 50: State=(8, 3, 6, 8, 2, 8, 6, 1, 8), Reward=10.001\n",
      "Step 51: State=(8, 3, 6, 8, 2, 8, 6, 2, 8), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 9:\n",
      "State: [8, 6, 8, 6, 2, 4, 8, 1, 8]\n",
      "Reward: 10.001\n",
      "Discovered at training step: 1591\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 0, 0, 0, 1), Reward=0.001\n",
      "Step 2: State=(0, 0, 0, 0, 0, 0, 0, 0, 2), Reward=0.001\n",
      "Step 3: State=(0, 0, 0, 1, 0, 0, 0, 0, 2), Reward=0.001\n",
      "Step 4: State=(0, 1, 0, 1, 0, 0, 0, 0, 2), Reward=0.001\n",
      "Step 5: State=(0, 1, 0, 1, 0, 1, 0, 0, 2), Reward=0.001\n",
      "Step 6: State=(0, 1, 0, 1, 1, 1, 0, 0, 2), Reward=0.001\n",
      "Step 7: State=(0, 1, 0, 1, 1, 1, 0, 1, 2), Reward=0.001\n",
      "Step 8: State=(1, 1, 0, 1, 1, 1, 0, 1, 2), Reward=0.001\n",
      "Step 9: State=(1, 1, 1, 1, 1, 1, 0, 1, 2), Reward=0.001\n",
      "Step 10: State=(1, 1, 2, 1, 1, 1, 0, 1, 2), Reward=0.001\n",
      "Step 11: State=(1, 2, 2, 1, 1, 1, 0, 1, 2), Reward=0.001\n",
      "Step 12: State=(1, 2, 2, 1, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 13: State=(1, 3, 2, 1, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 14: State=(2, 3, 2, 1, 1, 1, 1, 1, 2), Reward=0.001\n",
      "Step 15: State=(2, 3, 2, 1, 1, 1, 1, 1, 3), Reward=0.001\n",
      "Step 16: State=(3, 3, 2, 1, 1, 1, 1, 1, 3), Reward=0.001\n",
      "Step 17: State=(3, 3, 3, 1, 1, 1, 1, 1, 3), Reward=0.001\n",
      "Step 18: State=(4, 3, 3, 1, 1, 1, 1, 1, 3), Reward=0.001\n",
      "Step 19: State=(4, 4, 3, 1, 1, 1, 1, 1, 3), Reward=0.001\n",
      "Step 20: State=(4, 4, 3, 1, 1, 1, 2, 1, 3), Reward=0.001\n",
      "Step 21: State=(4, 4, 3, 1, 1, 1, 2, 1, 4), Reward=0.001\n",
      "Step 22: State=(4, 4, 3, 1, 1, 2, 2, 1, 4), Reward=0.001\n",
      "Step 23: State=(4, 4, 3, 2, 1, 2, 2, 1, 4), Reward=0.001\n",
      "Step 24: State=(4, 4, 3, 2, 1, 2, 3, 1, 4), Reward=0.001\n",
      "Step 25: State=(4, 4, 4, 2, 1, 2, 3, 1, 4), Reward=0.001\n",
      "Step 26: State=(4, 4, 4, 2, 1, 2, 4, 1, 4), Reward=0.001\n",
      "Step 27: State=(4, 4, 4, 2, 1, 3, 4, 1, 4), Reward=0.001\n",
      "Step 28: State=(4, 4, 4, 3, 1, 3, 4, 1, 4), Reward=0.001\n",
      "Step 29: State=(4, 4, 5, 3, 1, 3, 4, 1, 4), Reward=0.001\n",
      "Step 30: State=(4, 4, 5, 3, 1, 4, 4, 1, 4), Reward=0.001\n",
      "Step 31: State=(4, 4, 6, 3, 1, 4, 4, 1, 4), Reward=1.001\n",
      "Step 32: State=(4, 4, 6, 3, 1, 4, 4, 1, 5), Reward=1.001\n",
      "Step 33: State=(5, 4, 6, 3, 1, 4, 4, 1, 5), Reward=1.001\n",
      "Step 34: State=(5, 4, 7, 3, 1, 4, 4, 1, 5), Reward=0.001\n",
      "Step 35: State=(5, 4, 7, 3, 1, 4, 5, 1, 5), Reward=0.001\n",
      "Step 36: State=(6, 4, 7, 3, 1, 4, 5, 1, 5), Reward=1.001\n",
      "Step 37: State=(6, 4, 7, 4, 1, 4, 5, 1, 5), Reward=1.001\n",
      "Step 38: State=(6, 4, 7, 4, 1, 4, 6, 1, 5), Reward=2.001\n",
      "Step 39: State=(6, 4, 8, 4, 1, 4, 6, 1, 5), Reward=4.001\n",
      "Step 40: State=(6, 5, 8, 4, 1, 4, 6, 1, 5), Reward=4.001\n",
      "Step 41: State=(6, 5, 8, 4, 1, 4, 7, 1, 5), Reward=3.001\n",
      "Step 42: State=(7, 5, 8, 4, 1, 4, 7, 1, 5), Reward=2.001\n",
      "Step 43: State=(7, 5, 8, 4, 1, 4, 7, 1, 6), Reward=3.001\n",
      "Step 44: State=(7, 5, 8, 5, 1, 4, 7, 1, 6), Reward=3.001\n",
      "Step 45: State=(8, 5, 8, 5, 1, 4, 7, 1, 6), Reward=5.001\n",
      "Step 46: State=(8, 5, 8, 5, 2, 4, 7, 1, 6), Reward=5.001\n",
      "Step 47: State=(8, 5, 8, 6, 2, 4, 7, 1, 6), Reward=6.001\n",
      "Step 48: State=(8, 6, 8, 6, 2, 4, 7, 1, 6), Reward=7.001\n",
      "Step 49: State=(8, 6, 8, 6, 2, 4, 7, 1, 7), Reward=6.001\n",
      "Step 50: State=(8, 6, 8, 6, 2, 4, 7, 1, 8), Reward=8.001\n",
      "Step 51: State=(8, 6, 8, 6, 2, 4, 8, 1, 8), Reward=10.001\n",
      "------------------------------\n",
      "\n",
      "Mode 10:\n",
      "State: [8, 6, 6, 3, 6, 6, 6, 1, 8]\n",
      "Reward: 9.001\n",
      "Discovered at training step: 6\n",
      "\n",
      "Trajectory:\n",
      "Step 0: State=(0, 0, 0, 0, 0, 0, 0, 0, 0), Reward=0.000\n",
      "Step 1: State=(0, 0, 0, 0, 0, 1, 0, 0, 0), Reward=0.001\n",
      "Step 2: State=(0, 0, 0, 0, 0, 2, 0, 0, 0), Reward=0.001\n",
      "Step 3: State=(1, 0, 0, 0, 0, 2, 0, 0, 0), Reward=0.001\n",
      "Step 4: State=(1, 0, 0, 0, 0, 2, 0, 0, 1), Reward=0.001\n",
      "Step 5: State=(1, 1, 0, 0, 0, 2, 0, 0, 1), Reward=0.001\n",
      "Step 6: State=(1, 1, 1, 0, 0, 2, 0, 0, 1), Reward=0.001\n",
      "Step 7: State=(1, 1, 1, 1, 0, 2, 0, 0, 1), Reward=0.001\n",
      "Step 8: State=(1, 2, 1, 1, 0, 2, 0, 0, 1), Reward=0.001\n",
      "Step 9: State=(1, 2, 1, 1, 0, 2, 1, 0, 1), Reward=0.001\n",
      "Step 10: State=(1, 3, 1, 1, 0, 2, 1, 0, 1), Reward=0.001\n",
      "Step 11: State=(1, 3, 2, 1, 0, 2, 1, 0, 1), Reward=0.001\n",
      "Step 12: State=(1, 3, 2, 1, 0, 2, 1, 0, 2), Reward=0.001\n",
      "Step 13: State=(2, 3, 2, 1, 0, 2, 1, 0, 2), Reward=0.001\n",
      "Step 14: State=(2, 3, 2, 1, 0, 3, 1, 0, 2), Reward=0.001\n",
      "Step 15: State=(3, 3, 2, 1, 0, 3, 1, 0, 2), Reward=0.001\n",
      "Step 16: State=(3, 3, 2, 1, 0, 3, 1, 0, 3), Reward=0.001\n",
      "Step 17: State=(3, 3, 2, 1, 1, 3, 1, 0, 3), Reward=0.001\n",
      "Step 18: State=(3, 3, 2, 1, 1, 3, 1, 0, 4), Reward=0.001\n",
      "Step 19: State=(3, 4, 2, 1, 1, 3, 1, 0, 4), Reward=0.001\n",
      "Step 20: State=(3, 4, 2, 1, 1, 3, 2, 0, 4), Reward=0.001\n",
      "Step 21: State=(3, 4, 2, 1, 1, 4, 2, 0, 4), Reward=0.001\n",
      "Step 22: State=(4, 4, 2, 1, 1, 4, 2, 0, 4), Reward=0.001\n",
      "Step 23: State=(4, 4, 2, 1, 1, 4, 2, 0, 5), Reward=0.001\n",
      "Step 24: State=(5, 4, 2, 1, 1, 4, 2, 0, 5), Reward=0.001\n",
      "Step 25: State=(5, 4, 2, 1, 1, 4, 3, 0, 5), Reward=0.001\n",
      "Step 26: State=(5, 5, 2, 1, 1, 4, 3, 0, 5), Reward=0.001\n",
      "Step 27: State=(5, 5, 2, 1, 1, 4, 3, 0, 6), Reward=1.001\n",
      "Step 28: State=(5, 5, 3, 1, 1, 4, 3, 0, 6), Reward=1.001\n",
      "Step 29: State=(5, 5, 3, 1, 2, 4, 3, 0, 6), Reward=1.001\n",
      "Step 30: State=(5, 5, 3, 1, 2, 4, 3, 1, 6), Reward=1.001\n",
      "Step 31: State=(5, 5, 3, 1, 3, 4, 3, 1, 6), Reward=1.001\n",
      "Step 32: State=(5, 5, 3, 1, 3, 4, 4, 1, 6), Reward=1.001\n",
      "Step 33: State=(5, 5, 3, 1, 3, 4, 4, 1, 7), Reward=0.001\n",
      "Step 34: State=(5, 5, 3, 1, 3, 4, 5, 1, 7), Reward=0.001\n",
      "Step 35: State=(5, 5, 3, 1, 3, 4, 6, 1, 7), Reward=1.001\n",
      "Step 36: State=(5, 5, 3, 1, 3, 4, 6, 1, 8), Reward=3.001\n",
      "Step 37: State=(5, 5, 3, 2, 3, 4, 6, 1, 8), Reward=3.001\n",
      "Step 38: State=(5, 5, 4, 2, 3, 4, 6, 1, 8), Reward=3.001\n",
      "Step 39: State=(6, 5, 4, 2, 3, 4, 6, 1, 8), Reward=4.001\n",
      "Step 40: State=(6, 6, 4, 2, 3, 4, 6, 1, 8), Reward=5.001\n",
      "Step 41: State=(6, 6, 4, 2, 3, 5, 6, 1, 8), Reward=5.001\n",
      "Step 42: State=(6, 6, 4, 3, 3, 5, 6, 1, 8), Reward=5.001\n",
      "Step 43: State=(7, 6, 4, 3, 3, 5, 6, 1, 8), Reward=4.001\n",
      "Step 44: State=(7, 6, 5, 3, 3, 5, 6, 1, 8), Reward=4.001\n",
      "Step 45: State=(8, 6, 5, 3, 3, 5, 6, 1, 8), Reward=6.001\n",
      "Step 46: State=(8, 6, 5, 3, 4, 5, 6, 1, 8), Reward=6.001\n",
      "Step 47: State=(8, 6, 6, 3, 4, 5, 6, 1, 8), Reward=7.001\n",
      "Step 48: State=(8, 6, 6, 3, 5, 5, 6, 1, 8), Reward=7.001\n",
      "Step 49: State=(8, 6, 6, 3, 5, 6, 6, 1, 8), Reward=8.001\n",
      "Step 50: State=(8, 6, 6, 3, 6, 6, 6, 1, 8), Reward=9.001\n",
      "Step 51: State=(9, 6, 6, 3, 6, 6, 6, 1, 8), Reward=7.001\n",
      "------------------------------\n",
      "States shape: (2682, 9)\n",
      "First two states:\n",
      "[[8 2 8 0 6 8 8 3 6]\n",
      " [1 2 8 8 8 3 8 8 5]]\n",
      "Calculating PHATE...\n",
      "  Running PHATE on 2682 observations and 9 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.35 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.21 seconds.\n",
      "  Calculated graph and diffusion operator in 0.60 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 0.84 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 1.80 seconds.\n",
      "  Calculated landmark operator in 3.72 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 5\n",
      "  Calculated optimal t in 2.65 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.30 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 2.39 seconds.\n",
      "Calculated PHATE in 9.67 seconds.\n",
      "\n",
      "Modes visualization saved to runs/20250129_012240_tb_h256_l3_mr0.001_ts2000_d9_s51_er0.35_etFalse/visualization_modes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dimension Reduction on Modes\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from graph.dim_reduction import generate_visualizations\n",
    "\n",
    "run_dir = \"20250129_012240_tb_h256_l3_mr0.001_ts2000_d9_s51_er0.35_etFalse\"\n",
    "\n",
    "# Load the modes data\n",
    "modes_save_path = os.path.join(\"runs\", run_dir, \"modes_with_trajectories.pkl\")\n",
    "with open(modes_save_path, 'rb') as f:\n",
    "    modes_dict = pickle.load(f)\n",
    "print(f\"Total number of modes found: {len(modes_dict)}\\n\")\n",
    "\n",
    "# Sort modes by reward\n",
    "sorted_modes = sorted(modes_dict.items(), key=lambda x: x[1]['reward'], reverse=True) # highest to lowest\n",
    "print(\"Top 10 modes:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (state, info) in enumerate(sorted_modes[:10], 1):\n",
    "    print(f\"\\nMode {i}:\")\n",
    "    print(f\"State: {list(state)}\")\n",
    "    print(f\"Reward: {info['reward']:.3f}\")\n",
    "    print(f\"Discovered at training step: {info['step']}\")\n",
    "    print(\"\\nTrajectory:\")\n",
    "    for step, (s, r) in enumerate(zip(info['states'], info['rewards'])):\n",
    "        print(f\"Step {step}: State={s}, Reward={r:.3f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Extract states and rewards for visualization\n",
    "states = np.array([list(state) for state, _ in sorted_modes])\n",
    "print(f\"States shape: {states.shape}\")\n",
    "print(f\"First two states:\\n{states[:2]}\")\n",
    "\n",
    "rewards = np.array([info['reward'] for _, info in sorted_modes])\n",
    "\n",
    "# Create visualization directory if it doesn't exist\n",
    "vis_dir = \"visualization_modes\"\n",
    "save_path = os.path.join(\"runs\", run_dir, vis_dir)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "generate_visualizations(states, rewards, save_path)\n",
    "print(f\"\\nModes visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of modes found: 2682\n",
      "\n",
      "Mode 1 final reward: 10.001\n",
      "Trajectory length: 52\n",
      "\n",
      "Mode 2 final reward: 10.001\n",
      "Trajectory length: 52\n",
      "\n",
      "Mode 3 final reward: 10.001\n",
      "Trajectory length: 52\n",
      "\n",
      "Calculating PHATE...\n",
      "  Running PHATE on 156 observations and 9 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.02 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.04 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 2\n",
      "  Calculated optimal t in 0.02 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 0.01 seconds.\n",
      "Calculated PHATE in 0.08 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannyhuang/opt/anaconda3/envs/gflownet/lib/python3.8/site-packages/graphtools/graphs.py:108: UserWarning: Cannot set knn (300) to be greater than n_samples - 2 (154). Setting knn=154\n",
      "  warnings.warn(\n",
      "/Users/dannyhuang/opt/anaconda3/envs/gflownet/lib/python3.8/site-packages/graphtools/graphs.py:283: RuntimeWarning: Detected zero distance between samples 0 and 52, 0 and 104, 52 and 104, 53 and 105. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined trajectory visualization saved to runs/20250129_012240_tb_h256_l3_mr0.001_ts2000_d9_s51_er0.35_etFalse/visualization_trajectories\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dimension Reduction on combined Trajectories\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from graph.dim_reduction import generate_visualizations\n",
    "\n",
    "# Set number of top modes to analyze\n",
    "n_top_modes = 3\n",
    "run_dir = \"20250129_012240_tb_h256_l3_mr0.001_ts2000_d9_s51_er0.35_etFalse\"\n",
    "\n",
    "# Load the modes data\n",
    "modes_save_path = os.path.join(\"runs\", run_dir, \"modes_with_trajectories.pkl\")\n",
    "with open(modes_save_path, 'rb') as f:\n",
    "    modes_dict = pickle.load(f)\n",
    "print(f\"Total number of modes found: {len(modes_dict)}\\n\")\n",
    "\n",
    "# Sort modes by reward\n",
    "sorted_modes = sorted(modes_dict.items(), key=lambda x: x[1]['reward'], reverse=True) # highest to lowest\n",
    "\n",
    "# Get trajectories for top N modes\n",
    "all_states = []\n",
    "all_rewards = []\n",
    "for i, (state, info) in enumerate(sorted_modes[:n_top_modes]):\n",
    "    trajectory_states = info['states']\n",
    "    trajectory_rewards = info['rewards']\n",
    "    all_states.extend(trajectory_states)\n",
    "    all_rewards.extend(trajectory_rewards)\n",
    "    print(f\"Mode {i+1} final reward: {info['reward']:.3f}\")\n",
    "    print(f\"Trajectory length: {len(trajectory_states)}\\n\")\n",
    "\n",
    "# Combine all trajectories into single arrays\n",
    "all_states = np.array(all_states)\n",
    "all_rewards = np.array(all_rewards)\n",
    "\n",
    "# Create visualization directory if it doesn't exist\n",
    "vis_dir = \"visualization_trajectories\"\n",
    "save_path = os.path.join(\"runs\", run_dir, vis_dir)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Generate visualizations for combined trajectories\n",
    "generate_visualizations(all_states, all_rewards, save_path)\n",
    "print(f\"\\nCombined trajectory visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
