{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dannyhuang/gfn_test/discrete-gflownet\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n",
    "\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func, somitogenesis_reward_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run coord_reward_func: 0.000023800 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "# test_weights = [0, -10, 0, 4, 6, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10 \n",
    "# test_weights = [154, -200, 82, -33, 90, -90, -82, -13, 30] \n",
    "# reward = oscillator_reward_func(test_weights, plot=True)\n",
    "# print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e714e373e99b4c209cb51b8df0be2de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-10.0, description='w1', max=200.0, min=-200.0, step=1.0), FloatSliderâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(**kwargs)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# test_weights = [-10, -76, 11, -5, 50, 30, 12, -61, 86, -1, -33, 5, -32, 61, 13, 21, -30, 7, 6, 176, -16, 1, 7, 41, -75, 31, 65, -80, 51, 51, 25, -106, -13, 26, 60, -42, -200, -150, -76, -25, 180, -50, 15, -1, -200, -5, 55, 5, 6, 6, -4, 5, -10, -1, 2, 16]\n",
    "test_weights = [-10, -76, 11, -5, 50, 30, 12, -61, 86, -1, -33, 5, -32, 61, 13, 21, -30, 7, 6, 176, -16, 1, 7, 41, -75, 31, 65, -80, 51, 51, 25, -106, -13, 26, 60, -42, -200, -150, -76, -25, 180, -50, 15, -1, -200, -5, 55, 5, 6, 6, -4, 5, -10, -1, 2, 16]\n",
    "\n",
    "# test_weights = [80, -35, -155, 186, -20, -20]\n",
    "n_nodes = int((-1 + (1 + 4*len(test_weights))**0.5) / 2)  # solve quadratic: n^2 + n - len(state) = 0\n",
    "n_weights = n_nodes * n_nodes\n",
    "\n",
    "def update_plot(**kwargs):\n",
    "    params = list(kwargs.values())\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 12))\n",
    "    # n_weights = n_nodes * n_nodes\n",
    "    # weights = params[:n_weights]\n",
    "    draw_network_motif(params, ax=ax2)\n",
    "    ax2.set_title(f\"{n_nodes}-Node Network Motif\")\n",
    "    \n",
    "    # Plot somite pattern and get reward\n",
    "    start_time = time.perf_counter_ns()\n",
    "    reward = somitogenesis_reward_func(params, plot=True, ax=ax1)\n",
    "    print(f\"Reward for somitogenesis: {reward}\")\n",
    "    end_time = time.perf_counter_ns()\n",
    "    print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create sliders for all parameters\n",
    "sliders = {}\n",
    "# Weight sliders\n",
    "for i in range(n_weights):\n",
    "    default_value = test_weights[i] if i < len(test_weights) else 0\n",
    "    sliders[f'w{i+1}'] = FloatSlider(min=-200, max=200, step=1, value=default_value)\n",
    "# D value sliders    \n",
    "for i in range(n_nodes):\n",
    "    default_value = test_weights[n_weights+i] if n_weights+i < len(test_weights) else 0\n",
    "    sliders[f'd{i+1}'] = FloatSlider(min=-20, max=20, step=1, value=default_value)\n",
    "\n",
    "interact(update_plot, **sliders)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import plotting function from graph module\n",
    "# from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# # Define test weights to visualize\n",
    "# test_weights_list = [\n",
    "#     # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "#     [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ]\n",
    "\n",
    "# # Plot network motifs and their corresponding somite patterns\n",
    "# save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "# print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for Progressive Masking in GridEnv2\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from disc_gflownet.envs.grid_env2 import GridEnv2\n",
    "\n",
    "# Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "args = SimpleNamespace(\n",
    "    n_workers=1,\n",
    "    cache_max_size=1000,\n",
    "    min_reward=0.001,\n",
    "    custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "    actions_per_dim={'weight': [1, 5, 25, -1, -5, -25], 'diagonal': [1, 5, -1, -5]},\n",
    "    grid_bound={'weight': {'min': -200, 'max': 200}, 'diagonal': {'min': -20, 'max': 20}},\n",
    "    enable_time=False,\n",
    "    consistent_signs=True,\n",
    "    n_dims=3**2+3,  # 9 weights + 3 diagonals\n",
    "    n_steps=2+6+10,  # Total steps for all network sizes\n",
    "    steps_per_network={1:2, 2:6, 3:10}  # Steps per network size\n",
    ")\n",
    "\n",
    "env = GridEnv2(args)\n",
    "\n",
    "# Print all actions first\n",
    "env.print_actions()\n",
    "\n",
    "print(\"Testing Progressive Masking in GridEnv2\")\n",
    "print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "\n",
    "# s0\n",
    "env.reset()\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# First action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s1\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Second action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s2\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Third action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s3\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fourth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s4\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fifth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s5\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Sixth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s6\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Seventh action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s7\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Eighth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s8\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Ninth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s9\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Tenth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s10\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sparsity reward for sparse_state1: 0.6667\n",
      "Combined sparsity reward for sparse_state2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sparsity_reward_combined(state, w1=0.0, w2=1.0):\n",
    "    # Entropy-based component\n",
    "    # Normalize values to probabilities\n",
    "    abs_values = np.abs(state)\n",
    "    if sum(abs_values) == 0:\n",
    "        entropy_reward = 1.0  # maximum sparsity\n",
    "    else:\n",
    "        probs = abs_values / sum(abs_values)\n",
    "        # Calculate entropy (lower entropy = more sparse)\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "        entropy_reward = 1 / (1 + entropy)  # transform to reward\n",
    "    \n",
    "    # L0 component (explicitly rewards zeros)\n",
    "    n_zeros = sum(1 for x in state if x == 0)\n",
    "    l0_reward = n_zeros / len(state)\n",
    "    \n",
    "    return w1 * entropy_reward + w2 * l0_reward\n",
    "\n",
    "# Example states to test\n",
    "sparse_state1 = (10, 0, 0, 0, 10, 0, 0, 0, 10)\n",
    "sparse_state2 = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "# Test combined reward function\n",
    "print(f\"Combined sparsity reward for sparse_state1: {sparsity_reward_combined(sparse_state1):.4f}\")\n",
    "print(f\"Combined sparsity reward for sparse_state2: {sparsity_reward_combined(sparse_state2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
