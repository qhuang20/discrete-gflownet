{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dannyhuang/Developer/gflownet2/base\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n",
    "\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func, somitogenesis_reward_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run coord_reward_func: 0.000026375 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "# test_weights = [0, -10, 0, 4, 6, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10 \n",
    "# test_weights = [154, -200, 82, -33, 90, -90, -82, -13, 30] \n",
    "# reward = oscillator_reward_func(test_weights, plot=True)\n",
    "# print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b058b579d9942bfb02f26e9a4a7a930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=98.0, description='w1', min=-100.0, step=1.0), FloatSlider(value=-97.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(**kwargs)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "test_weights=(91, -58, -98, 38, 32, 62, -31, 64, 67, 40, 16, -31, -31, 60, -52, 56, -51, 25, 25, 1, -26, -17, -6, -5, 1, -6, 2, -51, 1, 2, -1, -25, -25, 6, -5, -25, 0, 50, 75, 0, 0, 76, -6, 0, -25, 50, -26, 0, -25, -95, -36, -16, -36, -1, 1, -1)\n",
    "\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 7, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -26, -1, 50, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, 0, 0)\n",
    "# , Reward=0.409\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, 0, 0)\n",
    "# , Reward=0.328\n",
    "\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, -25, 0)\n",
    "# , Reward=3.136\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -10, -25, 0)\n",
    "\n",
    "# test_weights= [81, 83, -77, -62, -32, 38, -10, 60, 93, 51, -3, -86, -60, -13, -32, 25, 26, 80, 13, 6, -26, -5, -2, 26, 56, 6, 3, 35, -1, 2, -2, 33, -31, -1, -35, -5, 100, 100, -100, -25, 25, -26, 25, 50, -100, 100, -100, 32, -100, 98, -38, -16, 51, 0, -15, 26]\n",
    "\n",
    "# test_weights= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# test_weights= [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# test_weights=[90, 100, -5, -75, -30, -10, 30, -75, 50, -5, 0, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 80, 50, 25, 0, 0, 0]\n",
    "# (note: not all nodes recieve mophorgen input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate number of nodes from the length of state vector\n",
    "n_nodes = int((-1 + (1 + 4*len(test_weights))**0.5) / 2)  # solve quadratic: n^2 + n - len(state) = 0\n",
    "n_weights = n_nodes * n_nodes\n",
    "\n",
    "def update_plot(**kwargs):\n",
    "    params = list(kwargs.values())\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 12))\n",
    "    \n",
    "    # Draw network motif in second subplot\n",
    "    draw_network_motif(params, ax=ax2)\n",
    "    ax2.set_title(f\"{n_nodes}-Node Network Motif\")\n",
    "    \n",
    "    # Plot somite pattern and get reward\n",
    "    start_time = time.perf_counter_ns()\n",
    "    reward = somitogenesis_reward_func(params, plot=True, ax=ax1)\n",
    "    end_time = time.perf_counter_ns()\n",
    "    print(f\"Reward for somitogenesis: {reward}\")\n",
    "    print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create sliders for all parameters\n",
    "sliders = {}\n",
    "# Weight sliders\n",
    "for i in range(n_weights):\n",
    "    default_value = test_weights[i] if i < len(test_weights) else 0\n",
    "    sliders[f'w{i+1}'] = FloatSlider(min=-100, max=100, step=1, value=default_value)\n",
    "# D value sliders    \n",
    "for i in range(n_nodes):\n",
    "    default_value = test_weights[n_weights+i] if n_weights+i < len(test_weights) else 0\n",
    "    sliders[f'd{i+1}'] = FloatSlider(min=-100, max=100, step=1, value=default_value)\n",
    "\n",
    "interact(update_plot, **sliders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import plotting function from graph module\n",
    "# from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# # Define test weights to visualize\n",
    "# test_weights_list = [\n",
    "#     # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "#     [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ]\n",
    "\n",
    "# # Plot network motifs and their corresponding somite patterns\n",
    "# save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "# print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for Progressive Masking in GridEnv2\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from disc_gflownet.envs.grid_env2 import GridEnv2\n",
    "\n",
    "# Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "args = SimpleNamespace(\n",
    "    n_workers=1,\n",
    "    cache_max_size=1000,\n",
    "    min_reward=0.001,\n",
    "    custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "    actions_per_dim={'weight': [1, 5, 25, -1, -5, -25], 'diagonal': [1, 5, -1, -5]},\n",
    "    grid_bound={'weight': {'min': -200, 'max': 200}, 'diagonal': {'min': -20, 'max': 20}},\n",
    "    enable_time=False,\n",
    "    consistent_signs=True,\n",
    "    n_dims=3**2+3,  # 9 weights + 3 diagonals\n",
    "    n_steps=2+6+10,  # Total steps for all network sizes\n",
    "    steps_per_network={1:2, 2:6, 3:10}  # Steps per network size\n",
    ")\n",
    "\n",
    "env = GridEnv2(args)\n",
    "\n",
    "# Print all actions first\n",
    "env.print_actions()\n",
    "\n",
    "print(\"Testing Progressive Masking in GridEnv2\")\n",
    "print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "\n",
    "# s0\n",
    "env.reset()\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# First action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s1\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Second action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s2\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Third action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s3\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fourth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s4\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fifth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s5\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Sixth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s6\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Seventh action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s7\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Eighth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s8\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Ninth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s9\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Tenth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s10\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sparsity_reward_combined(state, w1=0.0, w2=1.0):\n",
    "    # Entropy-based component\n",
    "    # Normalize values to probabilities\n",
    "    abs_values = np.abs(state)\n",
    "    if sum(abs_values) == 0:\n",
    "        entropy_reward = 1.0  # maximum sparsity\n",
    "    else:\n",
    "        probs = abs_values / sum(abs_values)\n",
    "        # Calculate entropy (lower entropy = more sparse)\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "        entropy_reward = 1 / (1 + entropy)  # transform to reward\n",
    "    \n",
    "    # L0 component (explicitly rewards zeros)\n",
    "    n_zeros = sum(1 for x in state if x == 0)\n",
    "    l0_reward = n_zeros / len(state)\n",
    "    \n",
    "    return w1 * entropy_reward + w2 * l0_reward\n",
    "\n",
    "# Example states to test\n",
    "sparse_state1 = (10, 0, 0, 0, 10, 0, 0, 0, 10)\n",
    "sparse_state2 = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "# Test combined reward function\n",
    "print(f\"Combined sparsity reward for sparse_state1: {sparsity_reward_combined(sparse_state1):.4f}\")\n",
    "print(f\"Combined sparsity reward for sparse_state2: {sparsity_reward_combined(sparse_state2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function with overflow protection\"\"\"\n",
    "    return 1 / (1 + np.exp( - np.clip(z, -500, 500)))\n",
    "\n",
    "# Test the sigmoid function with a simple differential equation\n",
    "# dx₁/dt = σ(d₁g(t) + w₁₁x₁ + w₁₂x₂ + w₁₃x₃) - s₁x₁\n",
    "\n",
    "def test_equation(x1, x2, x3, d1, w11, w12, w13, s1, g_value=1.0):\n",
    "    \"\"\"Calculate the right side of the differential equation for x1\"\"\"\n",
    "    z = d1 * g_value + w11 * x1 + w12 * x2 + w13 * x3\n",
    "    return sigmoid(z) - s1 * x1\n",
    "\n",
    "# Create interactive sliders to test the equation\n",
    "def update_equation(**kwargs):\n",
    "    x1 = kwargs['x1']\n",
    "    x2 = kwargs['x2']\n",
    "    x3 = kwargs['x3']\n",
    "    d1 = kwargs['d1']\n",
    "    w11 = kwargs['w11']\n",
    "    w12 = kwargs['w12']\n",
    "    w13 = kwargs['w13']\n",
    "    s1 = kwargs['s1']\n",
    "    g_value = kwargs['g']\n",
    "    \n",
    "    z = d1 * g_value + w11 * x1 + w12 * x2 + w13 * x3\n",
    "    # z= 0\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    dx1dt = sigmoid_z - s1 * x1\n",
    "    \n",
    "    print(f\"z = {z:.4f}\")\n",
    "    print(f\"σ(z) = {sigmoid_z:.4f}\")\n",
    "    # print(f\"dx₁/dt = {dx1dt:.4f}\")\n",
    "    \n",
    "    return sigmoid_z\n",
    "\n",
    "# Create sliders for all parameters\n",
    "equation_sliders = {\n",
    "    'x1': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₁'),\n",
    "    'x2': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₂'),\n",
    "    'x3': FloatSlider(min=0, max=1, step=0.1, value=0.5, description='x₃'),\n",
    "    'd1': FloatSlider(min=-10, max=10, step=0.5, value=1.0, description='d₁'),\n",
    "    'w11': FloatSlider(min=-10, max=10, step=0.5, value=0.0, description='w₁₁'),\n",
    "    'w12': FloatSlider(min=-10, max=10, step=0.5, value=0.0, description='w₁₂'),\n",
    "    'w13': FloatSlider(min=-10, max=100, step=0.5, value=0.0, description='w₁₃'),\n",
    "    's1': FloatSlider(min=0, max=2, step=0.1, value=1.0, description='s₁'),\n",
    "    'g': FloatSlider(min=0, max=1, step=0.1, value=1.0, description='g(t)')\n",
    "}\n",
    "\n",
    "\n",
    "interact(update_equation, **equation_sliders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test to understand how @ W.T works with a 2-node system\n",
    "n_nodes = 2\n",
    "\n",
    "# Create a simple 2x2 weight matrix\n",
    "W = np.array([\n",
    "    [1, 4],\n",
    "    [3, 2]\n",
    "])\n",
    "print(\"W:\\n\", W)\n",
    "print(\"W.T:\\n\", W.T)\n",
    "\n",
    "# Create a simple x array with 3 cells, 2 nodes each\n",
    "x = np.array([2, 3])  # Flattened array\n",
    "\n",
    "# # Reshape to (3 cells, 2 nodes)\n",
    "x_reshaped = x.reshape(-1, 2)\n",
    "print(\"x_reshaped:\\n\", x_reshaped)\n",
    "\n",
    "# # Calculate x_reshaped @ W.T\n",
    "result = x_reshaped @ W.T\n",
    "print(\"x_reshaped @ W.T:\\n\", result)\n",
    "\n",
    "# Alternative way using W @ x_reshaped.T\n",
    "result_alt = (W @ x_reshaped.T)\n",
    "print(\"\\nAlternative calculation:\")\n",
    "print(\"(W @ x_reshaped.T).T:\\n\", result_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
