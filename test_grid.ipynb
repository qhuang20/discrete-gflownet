{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dannyhuang/Developer/gflownet2/base\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n",
    "\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func, somitogenesis_reward_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run coord_reward_func: 0.000024791 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "# test_weights = [0, -10, 0, 4, 6, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10 \n",
    "# test_weights = [154, -200, 82, -33, 90, -90, -82, -13, 30] \n",
    "# reward = oscillator_reward_func(test_weights, plot=True)\n",
    "# print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d57a6e2aa944d4ac1728cc6001359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=80.0, description='w1', min=-100.0, step=1.0), FloatSlider(value=-35.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(**kwargs)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "test_weights = [80, -35, -155, 186, -20, -20]\n",
    "# test_weights = [0, 0, 0, 0, 0, 0]\n",
    "n_nodes = int((-1 + (1 + 4*len(test_weights))**0.5) / 2)  # solve quadratic: n^2 + n - len(state) = 0\n",
    "n_weights = n_nodes * n_nodes\n",
    "\n",
    "def update_plot(**kwargs):\n",
    "    params = list(kwargs.values())\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 12))\n",
    "    # n_weights = n_nodes * n_nodes\n",
    "    # weights = params[:n_weights]\n",
    "    draw_network_motif(params, ax=ax2)\n",
    "    ax2.set_title(f\"{n_nodes}-Node Network Motif\")\n",
    "    \n",
    "    # Plot somite pattern and get reward\n",
    "    start_time = time.perf_counter_ns()\n",
    "    reward = somitogenesis_reward_func(params, plot=True, ax=ax1)\n",
    "    print(f\"Reward for somitogenesis: {reward}\")\n",
    "    end_time = time.perf_counter_ns()\n",
    "    print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create sliders for all parameters\n",
    "sliders = {}\n",
    "# Weight sliders\n",
    "for i in range(n_weights):\n",
    "    default_value = test_weights[i] if i < len(test_weights) else 0\n",
    "    sliders[f'w{i+1}'] = FloatSlider(min=-100, max=100, step=1, value=default_value)\n",
    "# D value sliders    \n",
    "for i in range(n_nodes):\n",
    "    default_value = test_weights[n_weights+i] if n_weights+i < len(test_weights) else 0\n",
    "    sliders[f'd{i+1}'] = FloatSlider(min=-10, max=10, step=1, value=default_value)\n",
    "\n",
    "interact(update_plot, **sliders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import plotting function from graph module\n",
    "# from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# # Define test weights to visualize\n",
    "# test_weights_list = [\n",
    "#     # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "#     [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ]\n",
    "\n",
    "# # Plot network motifs and their corresponding somite patterns\n",
    "# save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "# print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "All available actions (action_names): 66\n",
      "['(0): weight0 1', '(1): weight0 5', '(2): weight0 25', '(3): weight0 -1', '(4): weight0 -5', '(5): weight0 -25', '(6): weight1 1', '(7): weight1 5', '(8): weight1 25', '(9): weight1 -1', '(10): weight1 -5', '(11): weight1 -25', '(12): weight2 1', '(13): weight2 5', '(14): weight2 25', '(15): weight2 -1', '(16): weight2 -5', '(17): weight2 -25', '(18): weight3 1', '(19): weight3 5', '(20): weight3 25', '(21): weight3 -1', '(22): weight3 -5', '(23): weight3 -25', '(24): weight4 1', '(25): weight4 5', '(26): weight4 25', '(27): weight4 -1', '(28): weight4 -5', '(29): weight4 -25', '(30): weight5 1', '(31): weight5 5', '(32): weight5 25', '(33): weight5 -1', '(34): weight5 -5', '(35): weight5 -25', '(36): weight6 1', '(37): weight6 5', '(38): weight6 25', '(39): weight6 -1', '(40): weight6 -5', '(41): weight6 -25', '(42): weight7 1', '(43): weight7 5', '(44): weight7 25', '(45): weight7 -1', '(46): weight7 -5', '(47): weight7 -25', '(48): weight8 1', '(49): weight8 5', '(50): weight8 25', '(51): weight8 -1', '(52): weight8 -5', '(53): weight8 -25', '(54): diag0 1', '(55): diag0 5', '(56): diag0 -1', '(57): diag0 -5', '(58): diag1 1', '(59): diag1 5', '(60): diag1 -1', '(61): diag1 -5', '(62): diag2 1', '(63): diag2 5', '(64): diag2 -1', '(65): diag2 -5']\n",
      "------------------------------------------\n",
      "Testing Progressive Masking in GridEnv2\n",
      "Environment has 3 nodes, 12 dimensions\n",
      "Action space size: 66\n",
      "Current network size: 1\n",
      "Steps Total: 0\n",
      "_step in current network: 0\n",
      "Number of allowed actions: 10\n",
      "Allowed action indices: [ 0  1  2  3  4  5 54 55 56 57]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 1\n",
      "Steps Total: 1\n",
      "_step in current network: 1\n",
      "Number of allowed actions: 7\n",
      "Allowed action indices: [ 0  1  2 54 55 56 57]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 2\n",
      "_step in current network: 0\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [3 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 3\n",
      "_step in current network: 1\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [4 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 4\n",
      "_step in current network: 2\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [5 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 5\n",
      "_step in current network: 3\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [6 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 6\n",
      "_step in current network: 4\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [7 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 2\n",
      "Steps Total: 7\n",
      "_step in current network: 5\n",
      "Number of allowed actions: 29\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 54 55 56\n",
      " 57 58 59 60 61]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [8 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 3\n",
      "Steps Total: 8\n",
      "_step in current network: 0\n",
      "Number of allowed actions: 63\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [9 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Done: False\n",
      "Current network size: 3\n",
      "Steps Total: 9\n",
      "_step in current network: 1\n",
      "Number of allowed actions: 63\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65]\n",
      "--------\n",
      "\n",
      "\n",
      "Taking action: 0\n",
      "New state: [10  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Done: False\n",
      "Current network size: 3\n",
      "Steps Total: 10\n",
      "_step in current network: 2\n",
      "Number of allowed actions: 63\n",
      "Allowed action indices: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65]\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test cases for Progressive Masking in GridEnv2\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from disc_gflownet.envs.grid_env2 import GridEnv2\n",
    "\n",
    "# Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "args = SimpleNamespace(\n",
    "    n_workers=1,\n",
    "    cache_max_size=1000,\n",
    "    min_reward=0.001,\n",
    "    custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "    actions_per_dim={'weight': [1, 5, 25, -1, -5, -25], 'diagonal': [1, 5, -1, -5]},\n",
    "    grid_bound={'weight': {'min': -200, 'max': 200}, 'diagonal': {'min': -20, 'max': 20}},\n",
    "    enable_time=False,\n",
    "    consistent_signs=True,\n",
    "    n_dims=3**2+3,  # 9 weights + 3 diagonals\n",
    "    n_steps=2+6+10,  # Total steps for all network sizes\n",
    "    steps_per_network={1:2, 2:6, 3:10}  # Steps per network size\n",
    ")\n",
    "\n",
    "env = GridEnv2(args)\n",
    "\n",
    "# Print all actions first\n",
    "env.print_actions()\n",
    "\n",
    "print(\"Testing Progressive Masking in GridEnv2\")\n",
    "print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "\n",
    "# s0\n",
    "env.reset()\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# First action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s1\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Second action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s2\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Third action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s3\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fourth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s4\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fifth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s5\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Sixth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s6\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Seventh action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s7\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Eighth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s8\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Ninth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s9\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Tenth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s10\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sparsity_reward_combined(state, w1=0.0, w2=1.0):\n",
    "    # Entropy-based component\n",
    "    # Normalize values to probabilities\n",
    "    abs_values = np.abs(state)\n",
    "    if sum(abs_values) == 0:\n",
    "        entropy_reward = 1.0  # maximum sparsity\n",
    "    else:\n",
    "        probs = abs_values / sum(abs_values)\n",
    "        # Calculate entropy (lower entropy = more sparse)\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "        entropy_reward = 1 / (1 + entropy)  # transform to reward\n",
    "    \n",
    "    # L0 component (explicitly rewards zeros)\n",
    "    n_zeros = sum(1 for x in state if x == 0)\n",
    "    l0_reward = n_zeros / len(state)\n",
    "    \n",
    "    return w1 * entropy_reward + w2 * l0_reward\n",
    "\n",
    "# Example states to test\n",
    "sparse_state1 = (10, 0, 0, 0, 10, 0, 0, 0, 10)\n",
    "sparse_state2 = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "# Test combined reward function\n",
    "print(f\"Combined sparsity reward for sparse_state1: {sparsity_reward_combined(sparse_state1):.4f}\")\n",
    "print(f\"Combined sparsity reward for sparse_state2: {sparsity_reward_combined(sparse_state2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
