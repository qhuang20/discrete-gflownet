{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dannyhuang/gfn_test/discrete-gflownet\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\".\") \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(sys.path[-1])\n",
    "\n",
    "\n",
    "from reward_func.evo_devo import coord_reward_func, oscillator_reward_func, somitogenesis_reward_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run coord_reward_func: 0.000024080 seconds\n",
      "Test reward for state (50, -53, -57, 8, 9, -6, -117, 81, 8): 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_state = (50, -53, -57, 8, 9, -6, -117, 81, 8)\n",
    "\n",
    "start_time = time.perf_counter_ns()\n",
    "test_reward = coord_reward_func(test_state)\n",
    "end_time = time.perf_counter_ns()\n",
    "print(f\"Time taken to run coord_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "\n",
    "print(f\"Test reward for state {test_state}: {test_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_weights = [-5, 2, 3, 1, 2, -1, 2, 1, -3] \n",
    "# test_weights = [0, -10, 0, 4, 6, -10, -10, 0, 0]  # w12=-5, w22=..., w23=-10, w31=-10 \n",
    "# test_weights = [154, -200, 82, -33, 90, -90, -82, -13, 30] \n",
    "# reward = oscillator_reward_func(test_weights, plot=True)\n",
    "# print(f\"Reward for oscillator: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c391ae16f640388fbc35d23b7bc5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w1', min=-100.0, step=1.0), FloatSlider(value=0.0, dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(**kwargs)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import plotting function from graph module\n",
    "from graph.graph import draw_network_motif\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "test_weights=(91, -58, -98, 38, 32, 62, -31, 64, 67, 40, 16, -31, -31, 60, -52, 56, -51, 25, 25, 1, -26, -17, -6, -5, 1, -6, 2, -51, 1, 2, -1, -25, -25, 6, -5, -25, 0, 50, 75, 0, 0, 76, -6, 0, -25, 50, -26, 0, -25, -95, -36, -16, -36, -1, 1, -1)\n",
    "\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 7, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -26, -1, 50, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, 0, 0)\n",
    "# , Reward=0.409\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, 0, 0)\n",
    "# , Reward=0.328\n",
    "\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -5, -25, 0)\n",
    "# , Reward=3.136\n",
    "test_weights=(98, -97, 60, 58, 17, -91, 31, 6, 32, 26, 85, -11, 5, 60, -26, -6, 25, 10, -1, 0, -30, -76, -1, 50, 40, 0, 0, 25, 0, -50, 0, 1, 5, 0, -5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, -32, 67, -2, -10, -25, 0)\n",
    "\n",
    "test_weights= [81, 83, -77, -62, -32, 38, -10, 60, 93, 51, -3, -86, -60, -13, -32, 25, 26, 80, 13, 6, -26, -5, -2, 26, 56, 6, 3, 35, -1, 2, -2, 33, -31, -1, -35, -5, 100, 100, -100, -25, 25, -26, 25, 50, -100, 100, -100, 32, -100, 98, -38, -16, 51, 0, -15, 26]\n",
    "\n",
    "test_weights= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "#BUG\n",
    "# test_weights= [100, -90, -100, -95, 15, 65, 40, -25, 30, 5, -5, -50, 0, 25, 25, -25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 40, 85, -25, 0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate number of nodes from the length of state vector\n",
    "n_nodes = int((-1 + (1 + 4*len(test_weights))**0.5) / 2)  # solve quadratic: n^2 + n - len(state) = 0\n",
    "n_weights = n_nodes * n_nodes\n",
    "\n",
    "def update_plot(**kwargs):\n",
    "    params = list(kwargs.values())\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 12))\n",
    "    \n",
    "    # Draw network motif in second subplot\n",
    "    draw_network_motif(params, ax=ax2)\n",
    "    ax2.set_title(f\"{n_nodes}-Node Network Motif\")\n",
    "    \n",
    "    # Plot somite pattern and get reward\n",
    "    start_time = time.perf_counter_ns()\n",
    "    reward = somitogenesis_reward_func(params, plot=True, ax=ax1)\n",
    "    end_time = time.perf_counter_ns()\n",
    "    print(f\"Reward for somitogenesis: {reward}\")\n",
    "    print(f\"Time taken to run somitogenesis_reward_func: {(end_time - start_time)/1e9:.9f} seconds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create sliders for all parameters\n",
    "sliders = {}\n",
    "# Weight sliders\n",
    "for i in range(n_weights):\n",
    "    default_value = test_weights[i] if i < len(test_weights) else 0\n",
    "    sliders[f'w{i+1}'] = FloatSlider(min=-100, max=100, step=1, value=default_value)\n",
    "# D value sliders    \n",
    "for i in range(n_nodes):\n",
    "    default_value = test_weights[n_weights+i] if n_weights+i < len(test_weights) else 0\n",
    "    sliders[f'd{i+1}'] = FloatSlider(min=-100, max=100, step=1, value=default_value)\n",
    "\n",
    "interact(update_plot, **sliders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import plotting function from graph module\n",
    "# from graph.graph import plot_network_motifs_and_somites\n",
    "\n",
    "# # Define test weights to visualize\n",
    "# test_weights_list = [\n",
    "#     # (65, -110, 52, -40, 32, -8, -65, -32, 71),\n",
    "#     [-5, 200, -6, -51, -26, 5, 5, 1, -125, 25, 30, 100, 0, 60, -5, -25, -1, 5, 0, -75, 30, 0, -1, -200, -25],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7],\n",
    "#     [60, 32, -38, -85, 70, -63, 22, -27, -7, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ]\n",
    "\n",
    "# # Plot network motifs and their corresponding somite patterns\n",
    "# save_path = plot_network_motifs_and_somites(test_weights_list)\n",
    "# print(f\"Plot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for Progressive Masking in GridEnv2\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from disc_gflownet.envs.grid_env2 import GridEnv2\n",
    "\n",
    "# Create a simple test environment with 3 nodes (12 dimensions: 9 weights + 3 diagonals)\n",
    "args = SimpleNamespace(\n",
    "    n_workers=1,\n",
    "    cache_max_size=1000,\n",
    "    min_reward=0.001,\n",
    "    custom_reward_fn=lambda x: 0,  # Dummy reward function\n",
    "    actions_per_dim={'weight': [1, 5, 25, -1, -5, -25], 'diagonal': [1, 5, -1, -5]},\n",
    "    grid_bound={'weight': {'min': -200, 'max': 200}, 'diagonal': {'min': -20, 'max': 20}},\n",
    "    enable_time=False,\n",
    "    consistent_signs=True,\n",
    "    n_dims=3**2+3,  # 9 weights + 3 diagonals\n",
    "    n_steps=2+6+10,  # Total steps for all network sizes\n",
    "    steps_per_network={1:2, 2:6, 3:10}  # Steps per network size\n",
    ")\n",
    "\n",
    "env = GridEnv2(args)\n",
    "\n",
    "# Print all actions first\n",
    "env.print_actions()\n",
    "\n",
    "print(\"Testing Progressive Masking in GridEnv2\")\n",
    "print(f\"Environment has {env.n_nodes} nodes, {env.n_dims} dimensions\")\n",
    "print(f\"Action space size: {env.action_dim}\")\n",
    "\n",
    "\n",
    "# s0\n",
    "env.reset()\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# First action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s1\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Second action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s2\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Third action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s3\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fourth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s4\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Fifth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s5\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Sixth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s6\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Seventh action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s7\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Eighth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s8\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Ninth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s9\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n",
    "\n",
    "# Tenth action\n",
    "action = allowed_indices[0]\n",
    "next_state, reward, done = env.step(action)\n",
    "print(\"\\nTaking action:\", action)\n",
    "print(f\"New state: {env._state}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# s10\n",
    "print(f\"Current network size: {env.current_network_size}\") \n",
    "print(f\"Steps Total: {env._step}\")\n",
    "print(f\"_step in current network: {env._step_in_current_network}\")\n",
    "mask = env.get_forward_mask(env._state)\n",
    "print(f\"Number of allowed actions: {np.sum(mask)}\")\n",
    "allowed_indices = np.where(mask)[0]\n",
    "print(f\"Allowed action indices: {allowed_indices}\")\n",
    "print(f\"--------\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sparsity_reward_combined(state, w1=0.0, w2=1.0):\n",
    "    # Entropy-based component\n",
    "    # Normalize values to probabilities\n",
    "    abs_values = np.abs(state)\n",
    "    if sum(abs_values) == 0:\n",
    "        entropy_reward = 1.0  # maximum sparsity\n",
    "    else:\n",
    "        probs = abs_values / sum(abs_values)\n",
    "        # Calculate entropy (lower entropy = more sparse)\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "        entropy_reward = 1 / (1 + entropy)  # transform to reward\n",
    "    \n",
    "    # L0 component (explicitly rewards zeros)\n",
    "    n_zeros = sum(1 for x in state if x == 0)\n",
    "    l0_reward = n_zeros / len(state)\n",
    "    \n",
    "    return w1 * entropy_reward + w2 * l0_reward\n",
    "\n",
    "# Example states to test\n",
    "sparse_state1 = (10, 0, 0, 0, 10, 0, 0, 0, 10)\n",
    "sparse_state2 = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "# Test combined reward function\n",
    "print(f\"Combined sparsity reward for sparse_state1: {sparsity_reward_combined(sparse_state1):.4f}\")\n",
    "print(f\"Combined sparsity reward for sparse_state2: {sparsity_reward_combined(sparse_state2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sigmoid function with clipping\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_with_clip(z):\n",
    "    \"\"\"Sigmoid activation function with overflow protection\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "# Test cases for different values of z\n",
    "test_values = [\n",
    "    -10000,  # Far below clip_min\n",
    "    -5000,   # At clip_min\n",
    "    -1000,   # Large negative value\n",
    "    -100,    # Moderate negative value\n",
    "    -1,      # Small negative value\n",
    "    0,       # Zero\n",
    "    1,       # Small positive value\n",
    "    100,     # Moderate positive value\n",
    "    1000,    # Large positive value\n",
    "    5000,    # At clip_max\n",
    "    10000    # Far above clip_max\n",
    "]\n",
    "\n",
    "# Calculate and print sigmoid values\n",
    "print(\"Testing sigmoid function with clipping (-5000, 5000):\")\n",
    "for z in test_values:\n",
    "    result = sigmoid_with_clip(z)\n",
    "    print(f\"sigmoid({z:8d}) = {result:.10f}\")\n",
    "\n",
    "# Compare with standard sigmoid (without clipping) for values within safe range\n",
    "print(\"\\nComparing with standard sigmoid (no clipping) for safe values:\")\n",
    "safe_values = [-10, -1, 0, 1, 10]\n",
    "for z in safe_values:\n",
    "    clipped = sigmoid_with_clip(z)\n",
    "    standard = 1 / (1 + np.exp(-z))\n",
    "    print(f\"z = {z:3d}: clipped = {clipped:.10f}, standard = {standard:.10f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
